---
layout: post
title:  "日志采集工具Logwatch"
date:   "2016-12-15 10:00:00"
---

* 2016年11月到12月中旬，开发了[Logwatch](https://github.com/peiliping/logwatch)的第一个版本。

* 去年公司生产环境下的应用日志采集，主要用的是Logstash，也就是ELK豪华套餐中的“L”。碰到的主要问题是：Cpu开销高、内存占用高（比如采集NginxAccessLog，NgxQPS 1000+ ，Logstash要消耗2G的内存，Cpu开销也有很大的波动）。

* 市面上与Logstash同类的开源项目有Flume、Heka、Fluentd、Graylog、Logkafka，以及各种Syslog和Collecter。不仅开源世界百花齐放，几乎每家云计算服务提供商还都提供日志采集、传输和搜索服务。

* 简单总结一下这些开源项目或多或少存在的一些问题：

> 1. 只支持单行Log，无法对Java应用中的ExceptionLog进行有效的采集和解析
> 2. 解析日志不够灵活
> 3. 性能一般
> 4. 不支持Kafka作为输出

* 自己造一个轮子要达到什么目标呢？

> 1. 代码要少，简单可控
> 2. 性能要好，不能因为采集日志影响了应用的性能
> 3. 支持新版Kafka(0.10+)
> 4. 支持单双行混合模式的应用日志
> 5. 对简单格式的日志可以不用编写正则表达式，根据类似nginx的logformat配置格式，自动生成解析正则,也就是logwatch里的grok功能

* Lua程序优化的心得

> 1. Lua的协程切换开销非常低，封装调度任务非常合适
> 2. Lua的正则性能非常好，其非贪婪匹配‘(.-)’，可以应用于非常多的解析场景，简单高效,控制好回溯，尽量明确字符类型
> 3. table要尽量复用，减少创建table和table.insert的过程中resize的开销
> 4. 不要反复做字符串拼接，必要时用table.concat替代
> 5. 尽量使用ipairs，少使用table.foreach
> 6. 尽量不要相信网上传说的Lua优化写法，Luajit已经优化了绝大多数问题了
> 7. Luajit有profile功能，可以用于测试代码热点
> 8. 程序运行一次很快，运行几亿次再看看性能如何
