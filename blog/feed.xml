<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pei LiPing's Blog</title>
    <description>Augur
</description>
    <link>http://peiliping.github.io/blog/</link>
    <atom:link href="http://peiliping.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 27 Jun 2018 13:59:32 +0800</pubDate>
    <lastBuildDate>Wed, 27 Jun 2018 13:59:32 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>618备战</title>
        <description>&lt;p&gt;在JD第一次参加了大促的备战工作，我所负责的系统也要应对618当天的流量洪峰。&lt;/p&gt;

&lt;p&gt;这半年开发的三个实时相关的工具都上线了，binlog采集、准实时hive表数据、日志分发。&lt;/p&gt;

&lt;p&gt;在618备战期间，我对这半年的开发有很多的思考，主要是平台运营、工具特性，性能与监控等方面。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;平台型和工具型&lt;/h2&gt;

&lt;p&gt;作为一个工具，你可以只关心功能，关心配置的灵活度，水平扩展能力和性能的极限。&lt;/p&gt;

&lt;p&gt;但是作为一个平台型的东西，这些是远远不够的。&lt;/p&gt;

&lt;p&gt;比如平台上的任务一定是绝大多数任务压力很小，有少数任务流量压力很大。&lt;/p&gt;

&lt;p&gt;业务上也会出现，某些任务只在一些特定的时间流量压力大，而其他时间流量很低。&lt;/p&gt;

&lt;p&gt;数据的表现上有条数多，或者单条体积大。甚至还要考虑有一些任务所使用的网络比其他任务要差一些。&lt;/p&gt;

&lt;p&gt;单从工具的角度来说，我们的三个tool的表现都非常优异，每种任务都可以使用3-4个性能档位配置，&lt;/p&gt;

&lt;p&gt;工具的配置非常灵活，暴露的可调节的参数也非常多，核心逻辑采用状态机等设计模式，&lt;/p&gt;

&lt;p&gt;非常好的兼容能力。性能上也做了诸多优化，binlog采集的性能甚至超过了正常从库同步的性能。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;临时解决问题&lt;/h2&gt;

&lt;p&gt;在任务上线的初期，经历了两周的阵痛，每天都有很多细碎的问题，比如某些任务延迟了，&lt;/p&gt;

&lt;p&gt;某些任务长时间无流量，某些任务经常报错等等。经过两周对监控的调整，大大降低了告警的次数，&lt;/p&gt;

&lt;p&gt;并且摸索出一套简单的运营办法，可以解决绝大多数问题。但是偶尔还是会有特例，&lt;/p&gt;

&lt;p&gt;在备战618的过程中，还是需要花非常多的时间去梳理核心任务和数据量大的任务，提前进行扩容。&lt;/p&gt;

&lt;p&gt;甚至在618前的一两个小时里，我们还在梳理增长迅猛的任务，并适当的扩容。&lt;/p&gt;

&lt;p&gt;这种工作无论你花多少时间，多么有耐心，还是会有遗漏的情况，因为这些都是实时数据或者&lt;/p&gt;

&lt;p&gt;准实时数据，临时人工处理一定已经晚了。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;基础准备&lt;/h2&gt;

&lt;p&gt;因为现实业务上的复杂性，所以我们的任务本身需要做一些基础的准备。&lt;/p&gt;

&lt;p&gt;第一、做好水平扩展能力，可以通过增加一个docker副本来实现性能的水平扩展。&lt;/p&gt;

&lt;p&gt;第二、当不能通过水平扩展提升能力时，可以通过增加资源来提供能力，比如cpu、mem。&lt;/p&gt;

&lt;p&gt;其实很多业务系统经常是增加了资源并不能提升处理能力，或者说不满足预期。&lt;/p&gt;

&lt;p&gt;第三、Job本身能够估算出自己的余量，这一点非常重要，能做到这一点的少之又少。&lt;/p&gt;

&lt;p&gt;第四、能够借助像docker、k8s等方式对运行时环境进行管理。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;未来&lt;/h2&gt;

&lt;p&gt;在产品端开发和运行时任务之间应该有一个运营质量管理系统，将监控和伸缩容量等结合起来，&lt;/p&gt;

&lt;p&gt;通过规则集或者AI相关技术，来解决需要大量人力时间的工作。&lt;/p&gt;

&lt;p&gt;在docker盛行的年代，传统运维方式发生了变化，监控也随之发生了变化，&lt;/p&gt;

&lt;p&gt;同样运营方法也在产生着巨大的变化，接来下几个月我会投入Flink相关的开发工作中，&lt;/p&gt;

&lt;p&gt;建设一个实时计算的平台，运营的实时化问题，会被我当成一个业务场景来对待，&lt;/p&gt;

&lt;p&gt;希望今年年底前能够彻底拜托人工运营的现状。&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jun 2018 20:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2018-06-16-618</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2018-06-16-618</guid>
        
        
        <category>618</category>
        
      </item>
    
      <item>
        <title>聊聊对channel的认识</title>
        <description>&lt;p&gt;今天聊一聊channel的话题，最近几年从事的工作内容都跟消息流和数据通道有关系，&lt;/p&gt;

&lt;p&gt;结合最近项目的调优，说说我对channel的认识。&lt;/p&gt;

&lt;h2 id=&quot;ringbuffer&quot;&gt;ringbuffer&lt;/h2&gt;

&lt;p&gt;说到channel，就不能不提disrupter的ringbuffer，最近几年非常热门的开源项目。&lt;/p&gt;

&lt;p&gt;ringbuffer强调对象的高度复用，减少GC的次数，有无数据的timeout的回调，方便实现batch逻辑，&lt;/p&gt;

&lt;p&gt;多种策略的选择，应对多种业务时效场景。&lt;/p&gt;

&lt;h2 id=&quot;selector&quot;&gt;selector&lt;/h2&gt;

&lt;p&gt;无论是实时流计算，还是消息通道，都会有一个类似selector的组件，基于某些字段对消息进行分类，&lt;/p&gt;

&lt;p&gt;将event投递到不同的channel中，主要是为了解决一定程度的顺序问题，&lt;/p&gt;

&lt;p&gt;比如将相同主键的消息顺序输出。flume中的selector就是针对header中的某个key进行mapping，&lt;/p&gt;

&lt;p&gt;实现了灵活强大的分流功能。当然使用key也尽量不要出现热点，要不然整体的消费能力就会提早到达瓶颈。&lt;/p&gt;

&lt;h2 id=&quot;broadcast&quot;&gt;broadcast&lt;/h2&gt;

&lt;p&gt;broadcast功能在channel中也是非常实用的，比如checkpoint相关的event向下游传递，如果下游有&lt;/p&gt;

&lt;p&gt;多个channel，那么就需要一个broadcast功能。在业务中broadcast的应用也非常广，比如上游计算&lt;/p&gt;

&lt;p&gt;导致了某种meta信息的变更，就可以通过broadcast的功能通知下游，这种方式比借助外部存储要高效的多，&lt;/p&gt;

&lt;p&gt;而且天然不用解决多version的问题。&lt;/p&gt;

&lt;h2 id=&quot;balance&quot;&gt;balance&lt;/h2&gt;

&lt;p&gt;如果你碰到了一拆N的channel场景，并且对数据的顺序不那么敏感时，肯定希望能最大限度的发挥下游&lt;/p&gt;

&lt;p&gt;channel的能力，很多人都在selector上选择roundrobin，或者是基于timestamp取模的方法，如果下游&lt;/p&gt;

&lt;p&gt;的消费能力非常强，这种做法是没有问题的。但如果你在下游sink是重IO类型的，这种做法就不会达到&lt;/p&gt;

&lt;p&gt;你想要的效果了。你会发现流经常会因为某一条下游的blocking，导致整条链路卡主，你使用了多条通道&lt;/p&gt;

&lt;p&gt;，但性能并没有成线性增长。这个问题在我的分发kafka消息至hdfs时就碰到了，一旦HDFS的响应速度下降&lt;/p&gt;

&lt;p&gt;整个job的吞吐量大大降低，增加channel并没有解决这个问题，只能增加jvm实例来解决，非常耗资源。&lt;/p&gt;

&lt;p&gt;这时，你需要的是类似负载均衡方面的WLC算法的解决方案。首先，你需要让下游的channel暴露出自己&lt;/p&gt;

&lt;p&gt;的余量。然后根据余量的情况选择最为空闲的节点，这样你增加的channel才会有意义。当然如果你有&lt;/p&gt;

&lt;p&gt;大量的broadcast消息，也会极大的影响你的性能。&lt;/p&gt;
</description>
        <pubDate>Thu, 10 May 2018 18:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2018-05-10-channel</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2018-05-10-channel</guid>
        
        
        <category>channel</category>
        
        <category>hdfs</category>
        
        <category>kafka</category>
        
      </item>
    
      <item>
        <title>项目问题总结</title>
        <description>&lt;p&gt;之前做的几个项目陆续上线了，碰到了不少稀奇古怪的问题，还有一些运营的问题，这里总结一下。&lt;/p&gt;

&lt;h2 id=&quot;mysql&quot;&gt;mysql字段类型&lt;/h2&gt;

&lt;p&gt;之前binlog测试都还算顺利，上线后发现了一些不常用的字段类型没有覆盖到，导致了一些数据异常。&lt;/p&gt;

&lt;h3 id=&quot;bit&quot;&gt;Bit&lt;/h3&gt;

&lt;p&gt;mysql的bit类型字段在binlog中解析出来默认是bitset，按照默认逻辑tostring的话会输出{0,1}。&lt;/p&gt;

&lt;p&gt;canal的处理是把bitset转成long型，也就是把bitset转为long就可以了。&lt;/p&gt;

&lt;h3 id=&quot;enum&quot;&gt;enum&lt;/h3&gt;

&lt;p&gt;mysql的enum类型之前我是从来没有用过的，这次也碰到了。enum类型的字段在JDBC中是char，&lt;/p&gt;

&lt;p&gt;所以需要在char类型中做一下区分。&lt;/p&gt;

&lt;h2 id=&quot;kafka-consumer&quot;&gt;监控kafka consumer&lt;/h2&gt;

&lt;p&gt;消费kafka都需要关注积压情况，以此来判断是否需要扩分区或者是消费者。&lt;/p&gt;

&lt;p&gt;普遍的做法是关注生产的offset和消费者的offset差，有不少开源的监控工具都是这样做的。&lt;/p&gt;

&lt;p&gt;然后根据经验设定一个阈值，超过了就告警。&lt;/p&gt;

&lt;p&gt;但是这个阈值的设定非常繁琐，阈值的大小与业务量的大小有关，跟提交offset的频率有关，&lt;/p&gt;

&lt;p&gt;跟监控程序扫描的频率有关，大多数阈值设定都非常大，灵敏度不高。&lt;/p&gt;

&lt;p&gt;我新上线的几个系统都是使用消费延迟时间来做告警的，在数据写入kafka（0.10以上的版本）的时候，&lt;/p&gt;

&lt;p&gt;将timestamp设定为当前时间，或者是一个业务时间。消费kafka的consumer，按照Partition做一个统计，&lt;/p&gt;

&lt;p&gt;consumer端将消费的数据时间戳和当前时间进行比对，来判断是否延迟，如果连续几分钟监控的延迟都&lt;/p&gt;

&lt;p&gt;大于10分钟，那么就认定程序当前消费能力不足，需要增加副本数来解决。&lt;/p&gt;

&lt;p&gt;如果你使用的kafka是0.8.2的话，没有timestamp的位置，可以通过key来传递。&lt;/p&gt;
</description>
        <pubDate>Sun, 22 Apr 2018 15:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2018-04-15-maintain</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2018-04-15-maintain</guid>
        
        
        <category>mysqlbinlog</category>
        
        <category>log</category>
        
        <category>hdfs</category>
        
        <category>kafka</category>
        
      </item>
    
      <item>
        <title>MysqlBinlog中的事务</title>
        <description>&lt;p&gt;binlog抽取的项目测试了一段时间，一直都很顺利，最近扩大了测试的规模，发现了一些细节问题。&lt;/p&gt;

&lt;h2 id=&quot;transactiontableid&quot;&gt;Transaction中的TableId&lt;/h2&gt;

&lt;p&gt;在binlog中有一个TableId字段，主要是用来关联TableMap和之后的RowEvent的。如果你在搜索引擎&lt;/p&gt;

&lt;p&gt;中搜索关于TableId的信息，会找到很多于此有关的Bug和故障，比如超过Int的最大值。这些细节就&lt;/p&gt;

&lt;p&gt;不再这里赘述了，总之在一个binlog局部片段中TableMap和RowEvent需要TableId进行关联。&lt;/p&gt;

&lt;p&gt;最初我们的代码是这样处理的，当遇到TableMapEvent就把它赋值给一个成员变量tablemap，&lt;/p&gt;

&lt;p&gt;遇到XIDEvent就让tablemap=null。这样处理其实是假定单一事务内只有一个tablemap，&lt;/p&gt;

&lt;p&gt;但这是不正确的。在一个事务里有可能碰到多个tablemap：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
tablemap(id=7000)

tablemap(id=7002)

rowdata(id=7000)

rowdata(id=7002)

xid

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这样就会导致第二条rowdata匹配了错误的tablemap，也就无法对数据进行有效的解析了。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;解决办法&lt;/h2&gt;

&lt;p&gt;为了解决这个问题我们翻看了canal和其他几个binlog处理的开源项目代码，大体解决方案为两类。&lt;/p&gt;

&lt;p&gt;1、用一个Hashmap把TableId和Tablemap缓存下来，当事务结束后，将mapclear掉。&lt;/p&gt;

&lt;p&gt;2、用一个全局的map将TableId和TableMapCache下来。&lt;/p&gt;

&lt;p&gt;方案二显然是有问题的，因为TableId并非有限范围，随着时间的累计会导致OOM。&lt;/p&gt;

&lt;p&gt;方案一是一个有效方案但是效率并不是特别高。&lt;/p&gt;

&lt;p&gt;在方案一的基础上我们做了一些改进：&lt;/p&gt;

&lt;p&gt;1、保留我们的tablemap成员对象，用来cache一个事务中的第一个tablemap，这个其实非常必要，&lt;/p&gt;

&lt;p&gt;这个tablemap的binlog位置会成为后面事务中其他数据的offset，可以作为有效的回退位点。&lt;/p&gt;

&lt;p&gt;2、再增加一个hashmap成员变量cache，当tablemap!=null并且当前event的type为tablemap时，&lt;/p&gt;

&lt;p&gt;判断TableId是不是相等的，如果不相等，将tablemap缓存到cache中，key为tableid。&lt;/p&gt;

&lt;p&gt;3、当event类型为row时，判断tableid与tablemap的tableid是不是相等，如果相等则用tablemap。&lt;/p&gt;

&lt;p&gt;如果不相等，就从cache中查找一个tablemap。&lt;/p&gt;

&lt;p&gt;4、当event类型为xid时，tablemap=null，cache.clear()&lt;/p&gt;

&lt;p&gt;这样在绝大多数情况下，不会启动hashmap，几乎不会产生额外的开销。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
 	EventType eventType = event.getHeader().getEventType();
        long dataTableId;
        switch (eventType) {
            case PRE_GA_WRITE_ROWS:
            case WRITE_ROWS:
            case EXT_WRITE_ROWS:
                dataTableId = ((WriteRowsEventData) event.getData()).getTableId();
                break;
            case PRE_GA_UPDATE_ROWS:
            case UPDATE_ROWS:
            case EXT_UPDATE_ROWS:
                dataTableId = ((UpdateRowsEventData) event.getData()).getTableId();
                break;
            case PRE_GA_DELETE_ROWS:
            case DELETE_ROWS:
            case EXT_DELETE_ROWS:
                dataTableId = ((DeleteRowsEventData) event.getData()).getTableId();
                break;
            case TABLE_MAP:
                dataTableId = ((TableMapEventData) event.getData()).getTableId();
                if (this.tableMapEvent == null) {
                    this.tableMapEvent = event;
                } else if (dataTableId != ((TableMapEventData) this.tableMapEvent.getData()).getTableId()) {
                    this.cacheTableMapEvent4Transaction.put(dataTableId, event);
                }
                this.binlogFileName = this.mysqlClient.getBinlogFilename();
                this.binlogPosition = ((EventHeaderV4) event.getHeader()).getPosition();
                getCounter().notRowSum.incrementAndGet();
                return;
            case XID:
                this.tableMapEvent = null;
                this.cacheTableMapEvent4Transaction.clear();
                getCounter().notRowSum.incrementAndGet();
                return;
            default:
                getCounter().notRowSum.incrementAndGet();
                return;
        }

        if (((TableMapEventData) this.tableMapEvent.getData()).getTableId() == dataTableId) {
            container.setPre(Validate.notNull(this.tableMapEvent));
        } else {
            Event e = this.cacheTableMapEvent4Transaction.get(dataTableId);
            container.setPre(Validate.notNull(e));
        }

        container.setCur(event);


&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 22 Mar 2018 13:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2018-03-22-binlogtransaction</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2018-03-22-binlogtransaction</guid>
        
        
        <category>mysqlbinlog</category>
        
        <category>transaction</category>
        
      </item>
    
      <item>
        <title>分发log数据到HDFS</title>
        <description>&lt;p&gt;将Kafka中的数据分发到HDFS上，提到这个需求首先想到的就是Flume。&lt;/p&gt;

&lt;p&gt;之前我也在Flume上做了很多改进来提高性能，因为整体框架的约束，只是修改一些皮毛。&lt;/p&gt;

&lt;p&gt;最近正好做了一个log分发的项目，作用和Flume非常相似，初步性能测试比Flume快很多。&lt;/p&gt;

&lt;p&gt;每秒钟可以从Kafka拉取300M的数据，写HDFS也大概了每秒100M。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;切换用户&lt;/h2&gt;

&lt;p&gt;HDFS一般都有权限管理，最常见的就是用用户名，假设写HDFS的程序所在的linux用户叫flume，&lt;/p&gt;

&lt;p&gt;那么HDFS上的文件用户名就是flume。为了方便管理我们的程序肯定是统一使用一个用户来启动的，&lt;/p&gt;

&lt;p&gt;这就需要切换  System.setProperty(“HADOOP_USER_NAME”, this.userName);&lt;/p&gt;

&lt;p&gt;在适当的时候执行以上语句，就可以达到切换用户的效果，当然还有其他办法,就不一一介绍了。&lt;/p&gt;

&lt;h2 id=&quot;codec&quot;&gt;Codec&lt;/h2&gt;

&lt;p&gt;绝大多数公司写入HDFS上的数据都会经过压缩，常见的如snappy，lzo，gzip等等，&lt;/p&gt;

&lt;p&gt;为了灵活方便，程序里通过配置来选择压缩工具，类似SPI的一种机制。&lt;/p&gt;

&lt;p&gt;其中值得注意的是lzo和lzop需要单独配置一下。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
List&amp;lt;Class&amp;lt;? extends CompressionCodec&amp;gt;&amp;gt; codecs = CompressionCodecFactory.getCodecClasses(conf);
codecs.add(LzoCodec.class);
codecs.add(LzopCodec.class);

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;sync&quot;&gt;sync&lt;/h2&gt;

&lt;p&gt;HDFS上一般一个block大小是128M左右，所以我们在写文件的过程中一定会sync一下，&lt;/p&gt;

&lt;p&gt;HDFS的FileSystem API中关于sync有好多种，我们参照Flume选择了hflush，性能还是非常不错的。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;分区&lt;/h2&gt;

&lt;p&gt;之前介绍过Flume的路径渲染极大影响了它的吞吐能力，在我们的程序中也极力避免复杂的路径渲染，&lt;/p&gt;

&lt;p&gt;比如单sink同时只能写一个文件等，但是分区路径的渲染还是避免不掉的，常见的分区有小时分区和天分区。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public enum PartitionType {

    DAY {
        @Override
        public PartitionRender newInstance() {
            return new DayPartitionRender();
        }
    }, HOUR {
        @Override
        public PartitionRender newInstance() {
            return new DayHourPartitionRender();
        }
    };

    public abstract PartitionRender newInstance();

    public abstract class PartitionRender {

        public abstract void render(StringBuilder base);

    }

    public class DayPartitionRender extends PartitionRender {

        private SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;/'dt'=yyyy-MM-dd/HH-mm-&quot;);

        @Override
        public void render(StringBuilder base) {
            base.append(simpleDateFormat.format(new Date()));
        }
    }

    public class DayHourPartitionRender extends PartitionRender {

        private SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;/'dt'=yyyy-MM-dd/'ht'=HH/HH-mm-&quot;);

        @Override
        public void render(StringBuilder base) {
            base.append(simpleDateFormat.format(new Date()));
        }
    }

}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;通过一个简单的枚举解决。&lt;/p&gt;

</description>
        <pubDate>Sun, 25 Feb 2018 13:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2018-02-25-hdfswriter</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2018-02-25-hdfswriter</guid>
        
        
        <category>hdfs</category>
        
        <category>log</category>
        
        <category>flume</category>
        
      </item>
    
      <item>
        <title>类型转换</title>
        <description>&lt;p&gt;之前几个月一直在忙一个binlog抽取的项目，将mysqlbinlog拉出来，写入kafka，&lt;/p&gt;

&lt;p&gt;之后消费kafka中的数据写到HDFS，文件格式为orcfile+snappy。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;上一篇blog讲过kafka中的数据是以avro为载体的，其中的数据字段存在一个map中，&lt;/p&gt;

&lt;p&gt;map的key和value都是charsequence，也就是说我们的数据在经过avro之后失去了类型信息。&lt;/p&gt;

&lt;p&gt;我们的目标端是写入hdfs上的orcFile，如果是写sequencefile，基本上我们就不关心类型了。&lt;/p&gt;

&lt;p&gt;阿里早期的数据仓库中，几乎所有的字段类型都是string的，这样做显然会有空间的浪费，&lt;/p&gt;

&lt;p&gt;但也比较方便，不容易出错，方便管理。在前一家公司里，我负责的离线数据都是以parquet为主的，&lt;/p&gt;

&lt;p&gt;orc相比parquet查询的性能会更快一些。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;类型转换&lt;/h2&gt;

&lt;p&gt;orc是有字段类型概念的，那么我们如何将string转成具体的类型呢？&lt;/p&gt;

&lt;p&gt;首先我们要知道原始类型（mysql中的类型），还要知道hive表中的字段类型（orc类型）。&lt;/p&gt;

&lt;p&gt;知道了这两端的类型，我们就有可能完成这个工作了。最简单的处理办法就是笛卡尔积。&lt;/p&gt;

&lt;p&gt;把每种组合的处理函数写好，进行配置就可以了。这样做的缺点非常明显，就是工作量大。&lt;/p&gt;

&lt;p&gt;如果有一天我们不再使用orc格式的类型，换成parquet或者其他的，那么还需要大量的重复工作。&lt;/p&gt;

&lt;p&gt;于是解决这个问题的关键是降低耦合度，降低复杂度。&lt;/p&gt;

&lt;p&gt;之前在阅读阿里开源的datax的时候看到过一个类似问题的解决方案，引入状态机。&lt;/p&gt;

&lt;p&gt;mysql的常见字段类型大概不到20种，为每种类型创建一个type类，并提供转成其他类型的方法。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public interface JavaType {

    Boolean toBoolean(String value);

    Integer toInt(String value);

    Long toLong(String value);

    Date toDate(String value);

    Float toFloat(String value);

    Double toDouble(String value);

    default String toString(String value) {
        return value;
    }

    default void unsupport() {
        throw new RuntimeException(&quot;type error&quot;);
    }

}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;碰到把boolean转date显然是不可能实现的，那么就直接unsupport好了。&lt;/p&gt;

&lt;p&gt;接下来解决orc端的问题&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; TINYINT(new C_LongColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createByte();
        }
    }, SMALLINT(new C_LongColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createShort();
        }
    }, INT(new C_LongColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createInt();
        }
    }, BIGINT(new C_LongColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createLong();
        }
    }, BOOLEAN(new C_BooleanColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createBoolean();
        }
    }, FLOAT(new C_DoubleColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createFloat();
        }
    }, DOUBLE(new C_DoubleColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createDouble();
        }
    }, DECIMAL(new C_DecimalColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createDecimal();
        }
    }, STRING(new C_BytesColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createString();
        }
    }, BINARY(new C_BytesColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createBinary();
        }
    }, CHAR(new C_BytesColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createChar();
        }
    }, VARCHAR(new C_BytesColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createVarchar();
        }
    }, TIMESTAMP(new C_TimeStampColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createTimestamp();
        }
    }, DATE(new C_DateColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createDate();
        }
    };

    private Convert convert;

    OrcTypeEnum(Convert convert) {
        this.convert = convert;
    }

    public static OrcTypeEnum findType(String type) {
        return OrcTypeEnum.valueOf(type.toUpperCase());
    }

    public abstract TypeDescription toOrcTypeDescption();

    public void setValue(ColumnVector vector, int row, String value, JavaType typeConvert) {
        convert.eval(vector, row, value, typeConvert);
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;为orc的每种类型创建一个convert，大概十来种的样子。&lt;/p&gt;

&lt;p&gt;通过这种方式，我们不仅解决了正常的类型转化需求，还能够天然支持date to long 和long to date这样的复杂需求，&lt;/p&gt;

&lt;p&gt;极大的提高了工具的灵活度。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;函数化&lt;/h2&gt;

&lt;p&gt;之前datax的方案是引入一个实体类对原始类型的数据进行包装，如果每一个字段都经过一次包装会严重增加体积，&lt;/p&gt;

&lt;p&gt;ygc的频率会提高，所以在我们的方案中是函数化的，相关类型进行编号，性能也有一些提升。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    public void writeData(List&amp;lt;String&amp;gt; result, List&amp;lt;ColumnInfo&amp;gt; columnInfos, List&amp;lt;JavaType&amp;gt; javaTypeList) {
        int row = this.batch.size++;
        for (int i = 0; i &amp;lt; result.size(); i++) {
            ColumnInfo col = columnInfos.get(i);
            col.getHiveTypeEnum().setValue(batch.cols[i], row, result.get(i), javaTypeList.get(col.getJavaTypeFunctionId()));
        }
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</description>
        <pubDate>Sat, 20 Jan 2018 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2018-01-20-typeconvert</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2018-01-20-typeconvert</guid>
        
        
        <category>type</category>
        
        <category>convert</category>
        
        <category>parser</category>
        
      </item>
    
      <item>
        <title>关于Avro的使用</title>
        <description>&lt;p&gt;14年开始尝试Flume的时候了解到Avro相关的东西，但一直都没有很深入的使用。&lt;/p&gt;

&lt;p&gt;最近在做Binlog采集时，写入Kafka的数据格式要求是Avro，在使用中了解碰到了一些小问题&lt;/p&gt;

&lt;p&gt;在这里分享一下。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;复用&lt;/h2&gt;

&lt;p&gt;序列化和反序列化时需要BinaryEncoder和BinaryDecoder，这个对象是可以反复使用的。&lt;/p&gt;

&lt;p&gt;反序列化接口是支持对象复用的，但是经过测试复用对象性能反而下降了。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;并发&lt;/h2&gt;

&lt;p&gt;序列化和反序列化都需要一个Json格式的Schema信息，或者叫Avsc。&lt;/p&gt;

&lt;p&gt;最开始多线程反序列化Avro数据时，是使用同一个Schema对象的，但是随着线程的增加，&lt;/p&gt;

&lt;p&gt;并发的效率提升并不明显，通过对线程状态的搜索，发现Schema对象上有Sync修饰，&lt;/p&gt;

&lt;p&gt;详见JsonProperties类。解决办法也很简单，每个线程使用一个独立的Schema对象就可以了。&lt;/p&gt;

&lt;h2 id=&quot;avrostring&quot;&gt;Avro中的String&lt;/h2&gt;

&lt;p&gt;Avro中的String可以是Java的String类型，还可以是使实现了CharSequence的UTF8。&lt;/p&gt;

&lt;p&gt;当然UTF8的Charsequence直接tostring就会转为Java的String了，但是Map的key就非常难处理。&lt;/p&gt;

&lt;p&gt;假如你的数据对象中有一个Map的field，并且Map的Key是String，通过get(“abc”)来获取&lt;/p&gt;

&lt;p&gt;value是不可行的，因为”abc”和UTF8(“abc”)的hashcode完全不同。只能遍历老的map，然后&lt;/p&gt;

&lt;p&gt;key.tostring()，再放入一个新的map中去，非常浪费性能。&lt;/p&gt;

&lt;p&gt;Avro是可以声明schema的string是java的string的来解决，还可以复写SpecificDatumReader&lt;/p&gt;

&lt;p&gt;这个类，特殊处理一下readMapKey方法，当是UTF8时调用一下tostring方法，转成java的string。&lt;/p&gt;
</description>
        <pubDate>Wed, 29 Nov 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-12-25-avro</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-12-25-avro</guid>
        
        
        <category>avro</category>
        
      </item>
    
      <item>
        <title>关于Mysql的Binlog抽取</title>
        <description>&lt;p&gt;这个月换了工作，双十一之前入职了京东大数据平台——实时数据组。&lt;/p&gt;

&lt;p&gt;着手关于MysqlBinlog相关系统的重构工作，这次就讲讲Binlog吧。&lt;/p&gt;

&lt;h2 id=&quot;binlog&quot;&gt;Binlog转实时数据流的开源动态&lt;/h2&gt;

&lt;p&gt;伪装MysqlSlave来获取MysqlBinlog的技术这几年已经非常普及了，很多中等规模的公司都有相关的&lt;/p&gt;

&lt;p&gt;基础技术组件，大多数是基于阿里几年前开源的Canal项目，在此基础上进行二次开发，将数据写入Kafka&lt;/p&gt;

&lt;p&gt;或者其他MQ系统中去，阿里云的RDS还提供类似的服务，也非常省心。&lt;/p&gt;

&lt;p&gt;17年年初的时候我也有意要做类似的项目，所以已经进行了最基础的技术选型和调研。&lt;/p&gt;

&lt;p&gt;Java系中最早的binlog开源项目有tungsten-replicator 、open-replicator等，后来都慢慢废弃了。&lt;/p&gt;

&lt;p&gt;之后Canal几乎统一了这个领域，但是阿里的这个开源项目最近几年的更新频率很一般。&lt;/p&gt;

&lt;p&gt;最近两年比较新的项目有mysql-time-machine，zendesk的maxwell都不错。&lt;/p&gt;

&lt;p&gt;shyiko的mysql-binlog-connector-java是一个BinlogClient的超精简内核，也被很多开源项目采用。&lt;/p&gt;

&lt;h2 id=&quot;mysqlbinlog&quot;&gt;Mysql的Binlog&lt;/h2&gt;

&lt;p&gt;Mysql的Binlog协议和格式我就不多介绍了，网上可以搜到很多。&lt;/p&gt;

&lt;p&gt;值得注意的时候，Mysql一定要配置成Row模式的，而且image需要配置成Full模式的。&lt;/p&gt;

&lt;p&gt;我采用shyiko的mysql-binlog-connector-java的方案进行了一下性能测试，可以达到80M/s。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;数据解析&lt;/h2&gt;

&lt;p&gt;shyiko的mysql-binlog-connector-java已经将数据解析成Java类型了，接下来只需要针对公司的规范&lt;/p&gt;

&lt;p&gt;进行数据的清洗和整理即可，最后转成标准序列化格式，比如AVRO、Pb、Json等。&lt;/p&gt;

&lt;p&gt;这中间性能损耗最大的地方就是序列化的开销了，要考虑异步或者多线程流等方式解决。&lt;/p&gt;

&lt;p&gt;在开发解析逻辑时碰到了不少细节问题，用shyiko的mysql-binlog-connector-java解析的数据与canal&lt;/p&gt;

&lt;p&gt;有一些细小的差别，比如datetime和timestamp字段解析的结果不一样，有可能受时区的影响；&lt;/p&gt;

&lt;p&gt;还有像text类型的字段是byte数组，需要自己指定charset转为string；decimal也有差异，需要使用&lt;/p&gt;

&lt;p&gt;numberformat进行格式化，group设置为false，并且setMinimumFractionDigits(1)。&lt;/p&gt;

&lt;h2 id=&quot;kafka&quot;&gt;写Kafka有序&lt;/h2&gt;

&lt;p&gt;将数据写到Kafka的速度是非常快的，但是因为Binlog的特殊性，需要一些设计，来保证数据的有序。&lt;/p&gt;

&lt;p&gt;Kafka的Client会把收到的数据根据meta进行进行重新组织，按照broker的使用情况进行分批发送。&lt;/p&gt;

&lt;p&gt;所以我们要根据业务情况指定Kafka的MessageKey，将相同业务含义的数据写到同一个Partition&lt;/p&gt;

&lt;p&gt;下，保证有序，乱序会导致最新值被老值覆盖。&lt;/p&gt;

&lt;p&gt;在写Kafka的过程中难免会有一些失败的情况发生，错误的重试机制由Kafka来保证，并且要强制设置&lt;/p&gt;

&lt;p&gt;ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION = 1，以防止重试时发生数据顺序错误。&lt;/p&gt;

&lt;h2 id=&quot;task&quot;&gt;Task的结构&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;P1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;B1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;P2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;B2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;P3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;单线程拉Binlog&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Ringbuffer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;单线程转化成AVRO对象&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Ringbuffer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;序列化并发送Kafka&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Ringbuffer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;序列化并发送Kafka&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;注释：P一般对应一个线程，B对应一个Disrupter的Ringbuffer。&lt;/p&gt;

&lt;p&gt;P1和P2环节的性能都非常好，所以采用的是单线程，阶段并行模式。&lt;/p&gt;

&lt;p&gt;P3阶段采用并行模式，P2处理好的数据根据一些key进行选择，路由到N个B2中去，一般可以用库表名称。&lt;/p&gt;

&lt;p&gt;压测时，这个Task跑起来之后可以让负载达到400%，基本上把Cpu资源跑满了。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;持久化位点信息&lt;/h2&gt;

&lt;p&gt;什么是位点信息？&lt;/p&gt;

&lt;p&gt;简单来说就是binlog的filename和offset，为了保证at least once，&lt;/p&gt;

&lt;p&gt;我们需要定期记录消费的位置，以便任务重启之后，继续消费，不丢数据（可有少量的重复）。&lt;/p&gt;

&lt;p&gt;如果P3阶段是单线程的，那么记录位点非常简单，提交kafka成功后，记录最后一条数据的offset，&lt;/p&gt;

&lt;p&gt;可以异步刷到远程存储中去，防止阻塞数据流。可是，现在P3阶段是多线程的，如何记录位点呢？&lt;/p&gt;

&lt;p&gt;定期让P2对所有的B2发送一条相同的含有位点信息的心跳包（位点值为P2最后处理的一条数据的位点）&lt;/p&gt;

&lt;p&gt;通过这种方式，将各个P3的信息进行更新，每次P3将数据成功发送到Kafka后就将其中的心跳包记录下来，&lt;/p&gt;

&lt;p&gt;作为一个可信的回退点，注意这个心跳包一定要在数据流中流转，起到挤压数据的作用，&lt;/p&gt;

&lt;p&gt;防止某些P3无数据的情况发生。相关代码参考我的sucker项目，这里就不介绍代码细节了。&lt;/p&gt;

&lt;p&gt;解决这个问题的思路源于Flink介绍文章中的一段话：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stream Barrier是Flink分布式Snapshotting中的核心元素，它会作为数据流的记录被同等看待，

被插入到数据流中，将数据流中记录的进行分组，并沿着数据流的方向向前推进。每个Barrier会携带

一个Snapshot ID，属于该Snapshot的记录会被推向该Barrier的前方。因为Barrier非常轻量，所以

并不会中断数据流。带有Barrier的数据流。

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 29 Nov 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-11-29-mysqlbinlog</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-11-29-mysqlbinlog</guid>
        
        
        <category>mysql</category>
        
        <category>binlog</category>
        
      </item>
    
      <item>
        <title>如何设计日活的流计算</title>
        <description>&lt;p&gt;之前在Redis上构建了基于Bitmap索引的存储结构，在Openresty上实现了一个简易的查询引擎，&lt;/p&gt;

&lt;p&gt;这里再思考一下数据的来源、采集、实时接入等问题。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;日活数据的来源&lt;/h2&gt;

&lt;p&gt;日活数据最大的可能来源就是浏览日志、登录日志等，数据的内容一般包括时间戳、UID、PageId等，&lt;/p&gt;

&lt;p&gt;一般网站或者App统计日活是需要区分模块和功能的，类似友盟、TalkingData等公司的产品，以一个&lt;/p&gt;

&lt;p&gt;SDK或者Agent的方式采集和发送数据。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;数据接收&lt;/h2&gt;

&lt;p&gt;客户端采集到的数据会定期发送回来，接收端并没有复杂的业务处理逻辑，基本上就是format。&lt;/p&gt;

&lt;p&gt;当接收端收到数据做了基本上的合法性校验之后，可以选择写日志或者发送到队列中（比如Kafka）。&lt;/p&gt;

&lt;p&gt;Openresty加Lua可以完成这部分的工作，性能会非常强悍，我比较倾向直接写本地Log，&lt;/p&gt;

&lt;p&gt;然后批量发送至Kafka。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;关于维度信息的计算&lt;/h2&gt;

&lt;p&gt;因为我们是要做维度数据统计的，所以在数据处理过程中就需要为访问日志数据添加维度的信息。&lt;/p&gt;

&lt;p&gt;维度信息大体可以分为两大类，一类是固定维度，一类是变动维度。固定维度就类似性别、年龄等，&lt;/p&gt;

&lt;p&gt;变动维度可以是所在的地理位置等信息。维度的数据经常是需要进行编码的，就是将string转id。&lt;/p&gt;

&lt;p&gt;客户端每次上传的数据中是冗余维度信息，还是在流计算中去获取转化丰富数据的维度，这个逻辑&lt;/p&gt;

&lt;p&gt;通常就是getOrCreate，如果是有一个Mysql的表来存储这种数据，再加一个Redis的Cache，这种&lt;/p&gt;

&lt;p&gt;中心化的设计在数据流量大的时候必然是瓶颈，所以要适当考虑在客户端做一些工作。比如，在客户&lt;/p&gt;

&lt;p&gt;端启动时，进行一些基本维度的转化工作，并且将这些转化的数据进行保存，在定期提交数据时，&lt;/p&gt;

&lt;p&gt;主动将数据的string转id的工作在客户端完成。当然这样的分散逻辑也导致了维护成本，当你需要进行&lt;/p&gt;

&lt;p&gt;一些数据的整理和升级时，就会要跟客户端打交道了，需要设计一下数据的Version和避免过于集中的&lt;/p&gt;

&lt;p&gt;数据更新导致后端压力过大。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;关于时间窗口与聚合&lt;/h2&gt;

&lt;p&gt;无论你选择了哪一种流计算框架，都会提供一个Timewindow的功能，提供基本的Map、Reduce功能。&lt;/p&gt;

&lt;p&gt;如果我们的浏览访问数据存在极大的聚合利益，可以压缩几倍或者十几倍，就值得一做。&lt;/p&gt;

&lt;p&gt;如果考虑到数据的修复和数据Delay等问题，一定要设计一个增量补全的逻辑，比如有put和incr操作。&lt;/p&gt;

&lt;p&gt;之前看过一些文章有在Hbase上加协处理器来完成的，当然我们的Redis+Lua也是可以的。&lt;/p&gt;

&lt;p&gt;数据的输出结果尽量不要直接写存储，可以再打回Kafka中，做多订阅的处理。Kafka的客户端可以把&lt;/p&gt;

&lt;p&gt;单条输出转化为批量，这种设计比较容易规避流计算的一些迭代输出单条的麻烦。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;维度数据的存储&lt;/h2&gt;

&lt;p&gt;每个用户的每次访问都带有20个维度信息，这种沉重的冗余信息会让你的存储不堪重负的。&lt;/p&gt;

&lt;p&gt;因为我们的目标是日活，也就是以用户为主体。当某个用户已经被处理过一次了，那么他的固定维度&lt;/p&gt;

&lt;p&gt;数据最好就不用再处理了，每次只处理变动信息就好了，表的行数相对比较稳定，跟总用户数有关。&lt;/p&gt;

&lt;p&gt;如果只处理日活相对比较简单，如果想要按照时间做区分，比如每小时一个列，相当于行转列。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;UID&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Gender&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Age&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;00:00&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;01:00&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;02:00&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;03:00&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;F&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;M&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;F&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;M&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section-5&quot;&gt;水平扩展&lt;/h2&gt;

&lt;p&gt;如果单机Redis不能存下所有的数据，就需要有一个分布式的方案，因为我们的计算是列级别的，&lt;/p&gt;

&lt;p&gt;所以同一个Uid的所有列必须在一台Redis实例中，于是我们的分布式方案只能按照Uid来分了。&lt;/p&gt;

&lt;p&gt;因为我们的UID是使用16K、64K的Block块来划分的，所以我们只要对Block块进行离散就好了。&lt;/p&gt;

&lt;p&gt;计算引擎的改动也会很小，这里就是一个简单的MapReduce的过程。&lt;/p&gt;
</description>
        <pubDate>Sun, 08 Oct 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-10-08-redis-bit3</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-10-08-redis-bit3</guid>
        
        
        <category>redis</category>
        
        <category>lua</category>
        
        <category>bitmap</category>
        
        <category>lpeg</category>
        
      </item>
    
      <item>
        <title>如何查询Redis中的Bitmap数据</title>
        <description>&lt;p&gt;书接上回，我们已经将日活的维度数据写入Redis的Bitmap结构中。&lt;/p&gt;

&lt;p&gt;今天再设计一个简单的查询引擎，通过表达式转化为Redis的指令集，最终得到日活的结果。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;设计一个简单的表达式&lt;/h2&gt;

&lt;p&gt;最通用的表达式就是SQL了，Hive、Drill、Presto、Phoenix等开源项目都在做类似的事情。&lt;/p&gt;

&lt;p&gt;我们这个筛选维度求日活的功能，没有那么复杂，所以简单一些就好。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(C1 = 3 &amp;amp; C2 = 1) &amp;amp; (C5 = 1 | C8 = 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;其中C(N)代表选择第几个维度，等号后面代表维度的值，&lt;/p&gt;

&lt;p&gt;“&amp;amp;”代表AND操作，“|”代表OR操作，括号表示结合顺序。&lt;/p&gt;

&lt;p&gt;这个表达式的含义我想很容易理解，跟四则混合运算差不多。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;表达式的解释器&lt;/h2&gt;

&lt;p&gt;下面看看我们如何解析这个表达式，将表达式转化为Redis执行的指令。&lt;/p&gt;

&lt;p&gt;之前玩了一段时间的Lpeg，也做过一些小东西，这里还是用Lpeg来做，&lt;/p&gt;

&lt;p&gt;在四则混合运算的例子上加以改进即可。&lt;/p&gt;

&lt;p&gt;下面是对表达式的定义。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function Blk(p)
  return p * V &quot;Space&quot;
end

local G = P{
    V &quot;Space&quot; * V &quot;Stmt&quot; ;
    Stmt      = Cf(V &quot;Group&quot; * Cg(V &quot;LogicSig&quot; * V &quot;Group&quot;)^0 , operation) ,
    Group     = V &quot;Element&quot; + V &quot;Open&quot; * V &quot;Stmt&quot; * V &quot;Close&quot; ,
    Element   = Cg(Blk(V &quot;ColNum&quot;) * Blk(V &quot;EqSignal&quot;) * Blk(V &quot;ColVal&quot;) / selector) ,
    LogicSig  = Blk(C(S &quot;&amp;amp;|&quot;)),

    ColNum    = P &quot;C&quot; * C(R &quot;09&quot;^1) ,
    EqSignal  = C(P &quot;=&quot;) ,
    ColVal    = C((R &quot;az&quot; + R &quot;AZ&quot; + R &quot;09&quot;)^1) ,

    Open      = Blk(P &quot;(&quot;) ,
    Close     = Blk(P &quot;)&quot;) ,
    Space     = S(&quot; \n\t&quot;)^0 ,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在解析到 C1=1 类似的结构时用selector函数处理&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function selector(colNum , op , colVal)
  lastKey = {&quot;b&quot; , colNum .. '-' .. colVal}
  return lastKey
end
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;解析到 XXX &amp;amp; YYY 或者 XXX | YYY 的结构时用operation函数处理&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function operation(left , op , right)
  local action = &quot;&quot;
  if op == &quot;&amp;amp;&quot; then 
    action = &quot;AND&quot;
  elseif op == &quot;|&quot; then
    action = &quot;OR&quot;
  end
  local tmpKey = &quot;__TMP-&quot; .. tmpCount
  lastKey = {&quot;t&quot; , tmpKey}
  table.insert(recycle , tmpKey)
  tmpCount = tmpCount + 1
  table.insert(process , {&quot;BITOP&quot; , action , tmpKey , left[1] , left[2] , right[1] , right[2]})
  return {&quot;t&quot; , tmpKey }
end
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;因为在Bitmap运算过程中，我们会产生一些临时的Bitmap数据，以__TMP开头的Key，&lt;/p&gt;

&lt;p&gt;这些Key在运算执行结束后，需要清理掉，我们将这些Key都记录在recycle数据里。&lt;/p&gt;

&lt;p&gt;下面看一个表达式解析的过程：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(C1 = 3 &amp;amp; C2 = 1) &amp;amp; (C5 = 1 | C8 = 6)

--------------------
C1	=	3
C2	=	1
table: 0x41760200	&amp;amp;	table: 0x417553e0
C5	=	1
C8	=	6
table: 0x417600f0	|	table: 0x41760178
table: 0x4175db68	&amp;amp;	table: 0x4175da68

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;再看看解析的结果：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lastKey : {&quot;t&quot; , &quot;__TMP-3&quot;}
process : [
           [&quot;BITOP&quot;,&quot;AND&quot;,&quot;__TMP-1&quot;,&quot;b&quot;,&quot;1-3&quot;,&quot;b&quot;,&quot;2-1&quot;],
           [&quot;BITOP&quot;,&quot;OR&quot;,&quot;__TMP-2&quot;,&quot;b&quot;,&quot;5-1&quot;,&quot;b&quot;,&quot;8-6&quot;],
           [&quot;BITOP&quot;,&quot;AND&quot;,&quot;__TMP-3&quot;,&quot;t&quot;,&quot;__TMP-1&quot;,&quot;t&quot;,&quot;__TMP-2&quot;],
           [&quot;BITCOUNT&quot;,[&quot;t&quot;,&quot;__TMP-3&quot;]]
          ]
recycle : [&quot;__TMP-1&quot;,&quot;__TMP-2&quot;,&quot;__TMP-3&quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;说明：其中的”t”代表是临时变量的key ， “b”代表的是需要结合Block块的序号的Key。&lt;/p&gt;

&lt;p&gt;如果完善一些的话，我们应该对逻辑表达式进行一下处理，也就是执行计划的优化。&lt;/p&gt;

&lt;p&gt;这里只是一个Demo，所以我们先跳过这一块，先把流程跑通。&lt;/p&gt;

&lt;h2 id=&quot;redis&quot;&gt;将执行计划推送给Redis&lt;/h2&gt;

&lt;p&gt;前一篇Blog，我们已经写了一个封装查询的LuaScript了，LuaScript会根据&lt;/p&gt;

&lt;p&gt;Nest结构中的数据，遍历所有的Block块，在每个Block块中进行，逻辑表达式的运算，&lt;/p&gt;

&lt;p&gt;最后把UV都SUM起来，就是返回的日活结果。&lt;/p&gt;

&lt;p&gt;这里我将process和recycle的数据都打包成了json(redis的lua可以使用cjson)。&lt;/p&gt;

&lt;p&gt;下面是重写的ReadScript&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;local scriptRead = redis:script(&quot;LOAD&quot; , 
[[
  local result = 0
  local tableName , dateTime = KEYS[1] , KEYS[2]
  local process , recycle    = cjson.decode(ARGV[1]) , cjson.decode(ARGV[2])
  local key4N = table.concat({tableName , &quot;-&quot; , dateTime , &quot;-Nest&quot;})

  for index , value in ipairs(process) do
    local params
    if value[1] == &quot;BITOP&quot; then
      params = {value[1] , value[2] , value[3]}
      for seq = 4 , #value , 2 do
        if value[seq] == &quot;b&quot; then
          table.insert(params , {tableName , &quot;-&quot; , dateTime , &quot;-&quot; , value[seq + 1] , &quot;-BMP-&quot; , 7})
        elseif value[seq] == &quot;t&quot; then
          table.insert(params , value[seq + 1])
        end
      end
    elseif value[1] == &quot;BITCOUNT&quot; then
      params = {value[1]}
      if value[2] == &quot;b&quot; then
          table.insert(params , {tableName , &quot;-&quot; , dateTime , &quot;-&quot; , value[3] , &quot;-BMP-&quot; , 7})
        elseif value[2] == &quot;t&quot; then
          table.insert(params , value[3])
        end
    end
    process[index] = params
  end
  
  local bytepos = 0
  local pos = redis.call(&quot;bitpos&quot; , key4N , 1 , bytepos)
  while (pos &amp;gt;= 0)
  do
    for g = pos , pos + 7 - bit.band(pos , 7) do
      local fill = redis.call(&quot;getbit&quot; , key4N , g)
      if fill == 1 then
        for index , value in ipairs(process) do
          local ps = {}
          for id , item in ipairs(value) do
            if type(item) == &quot;table&quot; then
              item[7] = g
              ps[id] = table.concat(item)
            else
              ps[id] = item
            end
          end
          if ps[1] == &quot;BITCOUNT&quot; then
            result = result + redis.call(unpack(ps))
          else
            redis.call(unpack(ps))
          end
        end
      end
    end
    bytepos = bit.rshift(pos , 3) + 1 
    pos = redis.call(&quot;bitpos&quot; , key4N , 1 , bytepos)
  end
  if #recycle &amp;gt; 0 then
    redis.call(&quot;del&quot; , unpack(recycle))
  end
  return result
]]
)

scripts:set(&quot;ReadScript&quot; , scriptRead)
ngx.say(&quot;ReadScript : &quot; .. scriptRead)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;最后我们再改写一些Openresty对查询接口的封装&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;G:match(expression)
table.insert(process , {&quot;BITCOUNT&quot; , unpack(lastKey)}) 

local r = redis:evalsha(scripts:get(&quot;ReadScript&quot;) , 2 , tableName , dateTime , json.encode(process) , json.encode(recycle))
ngx.say(r)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 20 Sep 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-09-20-redis-bit2</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-09-20-redis-bit2</guid>
        
        
        <category>redis</category>
        
        <category>lua</category>
        
        <category>bitmap</category>
        
        <category>lpeg</category>
        
      </item>
    
  </channel>
</rss>
