<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pei LiPing's Blog</title>
    <description>Augur
</description>
    <link>http://peiliping.github.io/blog/</link>
    <atom:link href="http://peiliping.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 28 Sep 2020 18:47:46 +0800</pubDate>
    <lastBuildDate>Mon, 28 Sep 2020 18:47:46 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>transaction</title>
        <description>&lt;p&gt;最近工作中大量的使用了Spring、Mybatis、JDBC的事务，这里稍作一下总结。&lt;/p&gt;

&lt;h2 id=&quot;springtransaction&quot;&gt;Spring的Transaction&lt;/h2&gt;

&lt;p&gt;现在Spring的Annotation就可以完成基本的事务配置，比起以前复杂的xml，确实方便了很多。&lt;/p&gt;

&lt;p&gt;网上能搜到很多介绍Spring-Transaction的，主要是讲事务的隔离级别和传播性，这里就不多说了。&lt;/p&gt;

&lt;p&gt;提醒大家去看看Spring-Transaction失效的N个原因，避免事务没有完整性的执行。&lt;/p&gt;

&lt;p&gt;我们犯的错是同class的非事务方法去调用事务方法，导致事务不起作用。&lt;/p&gt;

&lt;h2 id=&quot;jdbc&quot;&gt;JDBC事务&lt;/h2&gt;

&lt;p&gt;mysql有个参数是flush_tx_commit需要关注一下，可能会影响你的事务性能。&lt;/p&gt;

&lt;p&gt;事务中的操作肯定是串行的，所以还是尽量想办法让事务精简，能在事务外执行的尽量不要放进来。&lt;/p&gt;

&lt;p&gt;比如序列化、反序列化等耗时的操作，尽量提前准备好，再开启事务。单个事务的执行时间越短，&lt;/p&gt;

&lt;p&gt;多事务并发的性能才会更理想。在一些情况下，开启事务和逐个提交比起来会快一些，&lt;/p&gt;

&lt;p&gt;但不要为了追求这种性能而过分的扩大事务范围。锁的范围变大，大概率会与一些业务上的并发发生冲突，&lt;/p&gt;

&lt;p&gt;不利于整体性能的提升。&lt;/p&gt;

&lt;h2 id=&quot;mybatis&quot;&gt;Mybatis&lt;/h2&gt;

&lt;p&gt;这次项目中用的是mybatis-plus，在压测的时候关注了一下，性能确实很一般。&lt;/p&gt;

&lt;p&gt;但是因为项目本身的业务复杂性，直接使用jdbc确实很不方便。第一个性能问题就是分表plugin，&lt;/p&gt;

&lt;p&gt;做表名替换的开销比较大，从sql的参数中提取表名也可能比较慢。第二个性能问题就是通过mybatis&lt;/p&gt;

&lt;p&gt;处理数据，会导致比较频繁的ygc。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;最后&lt;/h2&gt;

&lt;p&gt;之前并没有太多关于事务类型业务的开发经验，这次优化能提升两三倍还是很满意的。&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Sep 2020 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2020-09-27-transaction</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2020-09-27-transaction</guid>
        
        
        <category>transaction</category>
        
      </item>
    
      <item>
        <title>ta4j2</title>
        <description>&lt;p&gt;上期讲了我在造一个类似ta4j的轮子，这次讲讲造轮子过程中的一些体会。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;关于回调的使用场景&lt;/h2&gt;

&lt;p&gt;在我的实现中，回调是贯穿始终的，我之前使用这种模式的次数不多，稍微总结一下。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;粗粒度的K线可以由细粒度的K线回调触发更新，非常方便回测实现。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;指标可以由K线或者其他指标的回调触发更新，保证数据的实时更新。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;当K线触发完所有指标的更新后，会触发Rule的回调，进行Rule的判断。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;规则判断后如果为True，可触发规则结果的回调，比如发消息、更新趋势、信号等。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-1&quot;&gt;多依赖指标的回调问题&lt;/h2&gt;

&lt;p&gt;某些指标的计算依赖多个其他指标的计算结果，于是更新这个指标的回调就有了路径问题，比如MACD。&lt;/p&gt;

&lt;p&gt;可以看看PairIndicatorSeries这个源码，构造时会进行指标的回调血缘分析。&lt;/p&gt;

&lt;p&gt;多依赖指标的回调，并不需要人为设定，根据分析结果可自动选择最佳的触发时机（最近的共同父节点）。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;指标重用问题&lt;/h2&gt;

&lt;p&gt;如果两个指标都使用了某一条K线的MA(20)作为构成的一部分，那么这个MA(20)是可以共用的。&lt;/p&gt;

&lt;p&gt;这样可以极大的减少计算量和内存开销，最典型的就是取K线的Close，极为常用。&lt;/p&gt;

&lt;p&gt;指标在某一时刻无论重复计算多少次，其结果都是一样的。当然重复计算我们也想尽量避免，&lt;/p&gt;

&lt;p&gt;在回调中提供了一个sequence，可以辅助去重。&lt;/p&gt;

&lt;p&gt;指标的构造和相同功能节点的自动复用，目前这块设计是有问题的，还需要重构打磨。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;规则重用问题&lt;/h2&gt;

&lt;p&gt;上面提到的指标是可以复用的，但规则绝对不可以，在Manager中有是否使用过的检查。&lt;/p&gt;

&lt;p&gt;Rule在某一时刻重复判断，其结果可能是不一样的，比如翻转规则、连续N次等。&lt;/p&gt;

&lt;p&gt;举几个复杂规则的例子，有同时判断2条指标的规则，比如快慢指标的上穿判断；&lt;/p&gt;

&lt;p&gt;还有由两个规则叠加起来组成的规则集，比如逻辑与运算。规则比指标要略微复杂一点。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;订单与仓位&lt;/h2&gt;

&lt;p&gt;订单的止盈止损也是个复杂问题，比如简单的止盈止损、追踪止盈止损等。&lt;/p&gt;

&lt;p&gt;在波动幅度较大的标的物上，追踪止盈止损并不能起到很好的效果，需要谨慎使用。&lt;/p&gt;

&lt;p&gt;控制盈利率与仓位比重是相对合理的做法，这样才能相对安全的达到收益最大化。&lt;/p&gt;

&lt;p&gt;在回测中，经常有人会为了收益最大化而过度调节止盈止损，基本上都是过拟合效果。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;最后&lt;/h2&gt;

&lt;p&gt;ta4j的话题就到此为止了，我个人认为简单的量化交易策略是不可能挣钱的，&lt;/p&gt;

&lt;p&gt;但是通过程序化的方式，做个辅助交易助手还是非常现实可行的。&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Aug 2020 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2020-08-18-ta4j2</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2020-08-18-ta4j2</guid>
        
        
        <category>ta4j</category>
        
      </item>
    
      <item>
        <title>ta4j</title>
        <description>&lt;p&gt;今天介绍一个比较有意思的java库叫&lt;a href=&quot;https://github.com/ta4j/ta4j&quot;&gt;ta4j&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;最近一段时间接触了一些跟程序化交易、自动交易有关的业务，每天跟k线、指标打交道，&lt;/p&gt;

&lt;p&gt;无意中在github上发现了一个叫ta4j的java库，虽然不是很成熟，但还是很有借鉴意义的。&lt;/p&gt;

&lt;p&gt;从综合成本上来说，不建议用java来搞程序化交易，python和nodejs更为合适一些，&lt;/p&gt;

&lt;p&gt;公共库更为成熟，而且更利于支持可视化的需求。&lt;/p&gt;

&lt;h2 id=&quot;ta4j&quot;&gt;ta4j主要提供了如下功能：&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;k线&lt;/li&gt;
    &lt;li&gt;指标&lt;/li&gt;
    &lt;li&gt;判断规则、规则的逻辑运算&lt;/li&gt;
    &lt;li&gt;策略&lt;/li&gt;
    &lt;li&gt;回测、报告&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section&quot;&gt;关于指标&lt;/h2&gt;

&lt;p&gt;k线的技术指标主要是基于k线值的算术运算得来的，比如均值、加权均值、差值、方差等。&lt;/p&gt;

&lt;p&gt;指标会随着k线的实时变化而变化，其表现形式跟K线是一样的，存储在一个时间序列数组里。&lt;/p&gt;

&lt;p&gt;ta4j在保存时间序列数据时采用的ArrayList，这个地方不是很好。当超过一定数量时，&lt;/p&gt;

&lt;p&gt;就需要淘汰最老的数据，会不断触发ArrayList的arraycopy，感觉换成ringbuffer更合理。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;关于规则、策略&lt;/h2&gt;

&lt;p&gt;基于K线和指标线的逻辑判断就是规则，比如在股票里我们常听说的多头排列。&lt;/p&gt;

&lt;p&gt;这种图形上的形态，需要转化为代码，成为一个识别规则(match)。ta4j里提供了一些常见的规则，&lt;/p&gt;

&lt;p&gt;比如交叉、向上突破、向下突破、阈值等等。&lt;/p&gt;

&lt;p&gt;因为在K线的使用和识别中，会应用到很多个规则的组合，所以规则需要支持逻辑运算(and/or/xor)。&lt;/p&gt;

&lt;p&gt;针对买入和卖出的一系列规则的集合，我们可以称之为策略，这个是整个程序交易的核心。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;关于回测&lt;/h2&gt;

&lt;p&gt;程序化交易里面回测是非常重要的一个环节，回测越接近真实，你的策略盈利的可能性就越大。&lt;/p&gt;

&lt;p&gt;最理想的回测就是将实时数据录制下来，这是最准的了。但是如此明细的数据是很难拿到的，&lt;/p&gt;

&lt;p&gt;大多数是用1min的k线替代明细数据。其实这样的数据是严重失真的，1min里的变化压缩为一个点。&lt;/p&gt;

&lt;p&gt;我一般的做法是将1min的k线点拆分为4个点，分别是开、高、低、收，依次写入数据集合进行测试。&lt;/p&gt;

&lt;p&gt;这样能尽量还原一个k线点的轨迹，比如一个K线点是上涨的，那就是开、低、高、收，依次变化。&lt;/p&gt;

&lt;p&gt;另外还要特别注意，在回测的时候用到的数据会不会是“未来的数据”，可能会让程序未卜先知。&lt;/p&gt;

&lt;p&gt;比如短线指标基于15min，中线是1hour的指标，在判断15min的指标规则时，1hour指标就是“未来”。&lt;/p&gt;

&lt;p&gt;我一般在回测的时候，其他粒度的k线会用1min的聚合而来，保持与1minK线的同步关系。&lt;/p&gt;

&lt;h2 id=&quot;watchdog-hubble&quot;&gt;watchdog-hubble&lt;/h2&gt;

&lt;p&gt;ta4j虽然给了我很多启发，但是问题还是非常多的。所以我自己造了一个轮子，放在watchdog下。&lt;/p&gt;

&lt;p&gt;结合上个月介绍的websocket和bark，我自己用起来还是非常顺手的。&lt;/p&gt;
</description>
        <pubDate>Mon, 20 Jul 2020 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2020-07-20-ta4j</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2020-07-20-ta4j</guid>
        
        
        <category>ta4j</category>
        
      </item>
    
      <item>
        <title>websocket</title>
        <description>&lt;p&gt;最近一个多月在和websocket打交道，这次来总结一下最近一阶段的工作吧。&lt;/p&gt;

&lt;h2 id=&quot;hydra&quot;&gt;hydra&lt;/h2&gt;

&lt;p&gt;三月份写了一篇关于netty的blog，介绍了我写的一个小项目叫&lt;a href=&quot;https://github.com/peiliping/hydra&quot;&gt;hydra&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;经过这几个月的完善，已经可以作为一个websocket的压测工具了。&lt;/p&gt;

&lt;p&gt;这两次压测实践中，单实例可以施压5W个并发连接数，满足大多数测试需求。&lt;/p&gt;

&lt;p&gt;通过参数可以配置心跳消息、订阅指令等，满足业务测试的需求。&lt;/p&gt;

&lt;h2 id=&quot;watchdog&quot;&gt;watchdog&lt;/h2&gt;

&lt;p&gt;使用netty作为websocket的client，如果解决重连重试的问题呢？&lt;/p&gt;

&lt;p&gt;在实际业务中，要考虑网络不稳定导致的websocket中断问题。websocket中断后，&lt;/p&gt;

&lt;p&gt;要重新建立连接，还要将业务的订阅指令重新发送一遍。&lt;/p&gt;

&lt;p&gt;在netty的中断分为两种情况，一个是创建连接就直接失败了，一种是在运行中突然中断。&lt;/p&gt;

&lt;p&gt;这两种情况分别有回调的接口可以捕获，而后触发延迟重连。可以参考&lt;a href=&quot;https://github.com/peiliping/watchdog&quot;&gt;watchdog&lt;/a&gt;的实现。&lt;/p&gt;

&lt;h2 id=&quot;bark&quot;&gt;Bark&lt;/h2&gt;

&lt;p&gt;最近无意间发现了一个iOS的App叫Bark，可以自定义手机通知提醒的消息。&lt;/p&gt;

&lt;p&gt;举个例子，你写了一个爬虫监控实时获取某只股票的价格，当他发生剧烈波动的时候，&lt;/p&gt;

&lt;p&gt;给自己的手机发送一条提醒，来提醒自己关注股票。那么如何给手机发送免费的消息呢？&lt;/p&gt;

&lt;p&gt;Bark就可以满足这个需求，安装Bark的App后，你会获得一个Url（包含一个uuid）。&lt;/p&gt;

&lt;p&gt;执行这个Url，你的手机就会收到来自Bark的通知消息了，消息的内容来自Url参数。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;提醒助手&lt;/h2&gt;

&lt;p&gt;将watchdog和Bark结合在一起，实现了一个小功能。&lt;/p&gt;

&lt;p&gt;订阅Huobi的BTC行情信息，经过计算，将满足剧烈波动条件的消息通过Bark发送到手机上。&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Jun 2020 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2020-06-29-websocket</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2020-06-29-websocket</guid>
        
        
        <category>websocket</category>
        
        <category>netty</category>
        
      </item>
    
      <item>
        <title>state</title>
        <description>&lt;p&gt;这次来介绍一下fevernova的state功能，与flink的state功能很类似，&lt;/p&gt;

&lt;p&gt;flink的state是保证恰好一次计算的核心点，当然至少一次也需要state。&lt;/p&gt;

&lt;h2 id=&quot;fevernovastate&quot;&gt;fevernova的state用途&lt;/h2&gt;

&lt;p&gt;之前fevernova都是作为数据传输框架，对state的依赖并不高。&lt;/p&gt;

&lt;p&gt;需要state用来存储kafka的offset，或者binlog的filename和position。&lt;/p&gt;

&lt;p&gt;在实现kafka To HDFS模块的时候，也会将关闭的文件列表和commit保存在state里，&lt;/p&gt;

&lt;p&gt;减少重复文件的产生，实现恰好一次。类似于flink的rollingfilesink。&lt;/p&gt;

&lt;h2 id=&quot;state&quot;&gt;state保存的位置&lt;/h2&gt;

&lt;p&gt;因为只保存少量的meta信息，所以state就简单的保存在文件系统里。&lt;/p&gt;

&lt;p&gt;如果需要提高可用性，就通过类似Nas的方式进行目录挂载。这类方案都是非常成熟的。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;计算状态&lt;/h2&gt;

&lt;p&gt;在无意中，发现了一个开源项目&lt;a href=&quot;https://github.com/OpenHFT&quot;&gt;OpenHFT&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;其中的项目质量还是比较高的，利用其中的序列化、持久化的库进行state的保存非常方便。&lt;/p&gt;

&lt;p&gt;让内存中保存状态的对象实现WriteBytesMarshallable和ReadBytesMarshallable接口，&lt;/p&gt;

&lt;p&gt;根据需求自定义序列化和反序列化的方法，就可以完成state的读写操作。&lt;/p&gt;

&lt;p&gt;简单测试了一下性能还是非常好的。fevernova中的exchange模块中有具体的使用。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;去重&lt;/h2&gt;

&lt;p&gt;单纯依靠Chandy-Lamport算法实现的流计算框架并不能在实际需求中完全实现恰好一次计算。&lt;/p&gt;

&lt;p&gt;比如写入kafka的数据有重复，通常就在计算逻辑中去重，这也意味着更多的cpu和内存开销。&lt;/p&gt;

&lt;p&gt;fevernova在exchange模块中实现了一个基于Roaringbitmap的滑窗过滤器，通过统一的去重。&lt;/p&gt;

&lt;p&gt;压缩的位图信息也会随着state进行保存，重启时可以完美衔接。&lt;/p&gt;
</description>
        <pubDate>Mon, 11 May 2020 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2020-05-11-state</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2020-05-11-state</guid>
        
        
        <category>state</category>
        
      </item>
    
      <item>
        <title>DB2DB</title>
        <description>&lt;p&gt;几年前写过一个叫meepo的小项目，用来解决一些临时导表的需求。&lt;/p&gt;

&lt;p&gt;很意外居然还有网友在git上提issue，看来需求还是很广泛的。&lt;/p&gt;

&lt;p&gt;meepo整体设计有很多问题，而且bug也比较多，不建议使用。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;改版重构&lt;/h2&gt;

&lt;p&gt;最近又碰到了类似的需求，从mysql的一种表抽取部分字段写到另外一张表。&lt;/p&gt;

&lt;p&gt;因为是定时数据处理，而且需要全量数据的导出，所以基于JDBC是最简单的方案。&lt;/p&gt;

&lt;p&gt;在数据摄取和分发部分，沿用了fevernova框架，这也比meepo更佳成熟稳定。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;流批一体&lt;/h2&gt;

&lt;p&gt;这两年有一个特别火的概念叫流批一体，flink社区也在努力统一数据底层模型。&lt;/p&gt;

&lt;p&gt;fevernova是一个类似flink的流式数据处理框架，之前也没试过做批处理。&lt;/p&gt;

&lt;p&gt;这次为了实现批处理的需求，为source增加了jobfinished的指令。&lt;/p&gt;

&lt;p&gt;当source完成预定处理，会等待一个checkpoint的周期，就结束任务了。&lt;/p&gt;

&lt;h2 id=&quot;rdb&quot;&gt;RDB&lt;/h2&gt;

&lt;p&gt;这次还需要支持postgre，虽然JDBC是通用的，但是Datasource和Sql拼装，&lt;/p&gt;

&lt;p&gt;都有很大的差别，需要重新设计扩展。&lt;/p&gt;

&lt;p&gt;举几个例子，table的scheme在mysql和postgre是不同的，在解决upsert场景的时候，&lt;/p&gt;

&lt;p&gt;mysql可以用replace，pg则要写on conflict子句。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;性能&lt;/h2&gt;

&lt;p&gt;这次性能优化并没有meepo那么激进，尽量做通用的优化，减少对依赖的侵入性修改。&lt;/p&gt;

&lt;p&gt;Postgre有一个unlogged类型的table，可以极大的提高写入速度，&lt;/p&gt;

&lt;p&gt;大家可以根据自己的需求酌情使用。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;规范&lt;/h2&gt;

&lt;p&gt;使用JDBC做增量数据导出，有一些需要注意的地方，比如：&lt;/p&gt;

&lt;p&gt;1、不能使用delete&lt;/p&gt;

&lt;p&gt;2、insert和update操作要更新updatetime字段&lt;/p&gt;

&lt;p&gt;3、需要对updatetime做索引&lt;/p&gt;

&lt;p&gt;4、有long型的自增主键，方便遍历&lt;/p&gt;
</description>
        <pubDate>Tue, 28 Apr 2020 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2020-04-28-DB2DB</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2020-04-28-DB2DB</guid>
        
        
        <category>jdbc</category>
        
        <category>database</category>
        
      </item>
    
      <item>
        <title>hydra</title>
        <description>&lt;p&gt;之前写过一篇关于netty的blog，经过几次梳理，hydra项目的代码基本稳定了。&lt;/p&gt;

&lt;p&gt;hydra项目分为client和server两部分。&lt;/p&gt;

&lt;h2 id=&quot;client&quot;&gt;client&lt;/h2&gt;

&lt;p&gt;client用于weboscket的功能和性能测试，短时间可以轻松建立50000个连接。&lt;/p&gt;

&lt;p&gt;可以通过参数指定subscribe的字符串，也可以定期发送指令维持heartbeat。&lt;/p&gt;

&lt;p&gt;如果server返回的数据是gzip压缩的binary数据，也是支持解压缩的。&lt;/p&gt;

&lt;p&gt;我尝试用client订阅了huobi的行情数据，配置启动参数就可以完成。其他同类网站也是没有问题的。&lt;/p&gt;

&lt;p&gt;当然不要用它来对别人的网站进行压力测试，会被封IP的。&lt;/p&gt;

&lt;h2 id=&quot;server&quot;&gt;server&lt;/h2&gt;

&lt;p&gt;server主要是一个推送服务的基础框架。&lt;/p&gt;

&lt;p&gt;推送的消息来源是从redis的topic中拉去的，采用redission的client。&lt;/p&gt;

&lt;p&gt;从消息中提取部分字段信息，构建nameSpace，通过ChannelManager进行推送。&lt;/p&gt;

&lt;p&gt;推送的消息格式，目前只支持text，未来需要考虑支持binary的压缩格式。&lt;/p&gt;

&lt;p&gt;除了以nameSpace为基础的推送外，还实现了基于Uid的推送模式，之前的blog已经介绍过原理了。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;websocket推送可以应用的领域非常广，弹幕、聊天室、股票行情软件等等。&lt;/p&gt;

&lt;p&gt;基于hydra的基础，后面要尝试一下netty的优化实践。&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Mar 2020 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2020-03-18-hydra</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2020-03-18-hydra</guid>
        
        
        <category>hydra</category>
        
        <category>netty</category>
        
      </item>
    
      <item>
        <title>时序索引</title>
        <description>&lt;p&gt;这次讲一个索引设计的例子，数据具有时序特征，比如，监控数据、股票k线数据等等。&lt;/p&gt;

&lt;p&gt;如果时序数据同时还具备多维度，那就需要druid、clickhouse来完成。&lt;/p&gt;

&lt;p&gt;像股票k线这样的数据，就没有太多的维度，按照股票的ID和K线的类型进行过滤即可。&lt;/p&gt;

&lt;p&gt;下面介绍一个股票K线数据索引的设计思路。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;时间戳&lt;/h2&gt;

&lt;p&gt;要做时序数据首先要定义时间字段，最简单的unixtime，精度到秒，比如1583769600。&lt;/p&gt;

&lt;p&gt;用4字节的int来存储unixtime，现在是可以存下的，未来还可以使用十几年，如果还不放心，&lt;/p&gt;

&lt;p&gt;可以考虑用无符号整形，足够用到退休了。&lt;/p&gt;

&lt;h2 id=&quot;k&quot;&gt;k线的类型&lt;/h2&gt;

&lt;p&gt;股票的K线主要是按照时间区分的，1min、3min、5min….一共十几种，1个字节就够了。&lt;/p&gt;

&lt;h2 id=&quot;id&quot;&gt;股票的ID&lt;/h2&gt;

&lt;p&gt;short对应的是两个字节的整形，最大可以到32768，无符号可以到65536。&lt;/p&gt;

&lt;p&gt;国内的A股也就四五千只股票的规模，考虑到可能退市等因素，2个字节也足够了。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;合并&lt;/h2&gt;

&lt;p&gt;为这三个字段建联合唯一索引，肯定是可以满足需求的，但是这样很铺张浪费。&lt;/p&gt;

&lt;p&gt;我的目标是用一个long型字段，来满足需求。long是8个字节，三个字段加起来是7个字节，&lt;/p&gt;

&lt;p&gt;还可以留下一个byte作为扩展。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
 ((long) this.symbolId &amp;lt;&amp;lt; 40) | ((long) this.type.value &amp;lt;&amp;lt; 32) | (long) this.timeSeq

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;合并成一个long型的字段，可以节省索引空间，高效的使用btree来处理Between，使用bitmap优化也没问题。&lt;/p&gt;

&lt;h2 id=&quot;query&quot;&gt;Query&lt;/h2&gt;

&lt;p&gt;在检索的时候，股票ID是确定的，k线类型也是确定的，时间是一个范围查询。&lt;/p&gt;

&lt;p&gt;将时间范围的起点和终点，通过上面的合并方法进行处理，可以得到两个long值，&lt;/p&gt;

&lt;p&gt;Between就可以搞定了。&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Feb 2020 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2020-02-24-timeindex</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2020-02-24-timeindex</guid>
        
        
        <category>index</category>
        
      </item>
    
      <item>
        <title>netty</title>
        <description>&lt;p&gt;新年第一篇，写点有关netty和websocket推送相关的。&lt;/p&gt;

&lt;p&gt;这些年用到netty的项目挺多的，但也一直没有仔细研究一下，最近在做websocket的测试，&lt;/p&gt;

&lt;p&gt;所以就用netty分别写了client端和server端，只是一个简单的应用。&lt;/p&gt;

&lt;h2 id=&quot;hydra&quot;&gt;hydra&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/peiliping/hydra&quot;&gt;Hydra&lt;/a&gt; 项目在这里，代码细节我就不介绍了，大家在网上都可以搜到。&lt;/p&gt;

&lt;h2 id=&quot;websocket&quot;&gt;关于websocket的压缩&lt;/h2&gt;

&lt;p&gt;我在性能测试过程中，曾经开启过WebSocketServerCompressionHandler，发生了缓慢内存泄露。&lt;/p&gt;

&lt;p&gt;经排查，Jvm堆内、外都没有问题，但free -m显示的内存剩余一直在降低，直到被OS-Kill掉。&lt;/p&gt;

&lt;p&gt;在netty的git-issue上搜索找到了相关信息（见ISSUE-9803）。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;推送&lt;/h2&gt;

&lt;p&gt;说到推送，肯定会提到socket.io这个项目，这几年非常火，值得研究一下。&lt;/p&gt;

&lt;p&gt;推送里有一个非常常见的问题，就是同一个账号的多端推送。&lt;/p&gt;

&lt;p&gt;举例：你在手机和网页都登录了，当你的账号有新的订单成交时，都应获得通知消息的推送。&lt;/p&gt;

&lt;p&gt;所以在推送服务器的内存中，就要维护一个Map，Key为Uid，Value是多端的Channels集合。&lt;/p&gt;

&lt;p&gt;在编写这段逻辑时，我希望尽量少的使用lock，但是uid所在的entry，需要保持原子性。&lt;/p&gt;

&lt;p&gt;如果手机端退出登录和网页端登录，在同一时刻发生，维护这个channels在map中的原子性就非常困难。&lt;/p&gt;

&lt;p&gt;解决方案有两种：&lt;/p&gt;

&lt;p&gt;1、当channels集合为空时，并不把它从map中remove掉，会导致Map中有一些垃圾信息。&lt;/p&gt;

&lt;p&gt;2、使用ConcurrentSkipListMap来替代Map&amp;lt;String,Set&lt;string&gt;&amp;gt;的嵌套结构。&lt;/string&gt;&lt;/p&gt;

&lt;p&gt;最后我选择了方案2，在Hydra项目中的Server模块ChannelManager中。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;结束&lt;/h2&gt;

&lt;p&gt;疫情还在继续，大家保重。&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Jan 2020 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2020-01-06-netty</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2020-01-06-netty</guid>
        
        
        <category>netty</category>
        
      </item>
    
      <item>
        <title>一点点总结</title>
        <description>&lt;p&gt;写Blog整整三年，虽然质量不高，但贵在坚持。&lt;/p&gt;

&lt;h2 id=&quot;avro&quot;&gt;关于avro的优化&lt;/h2&gt;

&lt;p&gt;之前写了一些关于avro通用数据格式的想法，这里再补充一点，实际应用中对schema进行定义，&lt;/p&gt;

&lt;p&gt;就意味着很高的管理成本，很多采用json或者hashmap为载体的方式可以非常灵活的增减字段，&lt;/p&gt;

&lt;p&gt;但是缺点非常明显，性能较差、丧失类型、数据体较大等。为了解决schema灵活的问题，我的想法&lt;/p&gt;

&lt;p&gt;是在avro的schema中定义一个int类型的字段保存schema的版本号或者hash值，在最后也增加一个&lt;/p&gt;

&lt;p&gt;bytes类型的字段，保存schema列表。这个列表信息可以用一些压缩手段来减少体积。&lt;/p&gt;

&lt;p&gt;schema的变更是比较低频的，所以不用每次都解析，一次解析后复用即可，在没有变更时可以忽略&lt;/p&gt;

&lt;p&gt;对这个schema信息的反序列化。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;关于背压&lt;/h2&gt;

&lt;p&gt;今年也写了好几篇文章来讲述我对背压的理解，无论是flink，还是自研的fregata。&lt;/p&gt;

&lt;p&gt;总体的思路就是去采集和统计，上游Operator在从buffer中getEvent的costtime。&lt;/p&gt;

&lt;p&gt;由于方法调用的耗时都很小，所以需要记录nanotime精度的耗时。&lt;/p&gt;

&lt;p&gt;在定制flink时，是依托于其一个wait的逻辑，只有在真正发生背压的时候才进行记录。&lt;/p&gt;

&lt;p&gt;但在fregata的自旋时间统计上有一个致命的缺陷，因为ringbuffer包装的原因，&lt;/p&gt;

&lt;p&gt;最终只是简单的统计了getNextSeq的方法调用时间。但是这里有一个缺陷，即使没有很明显的背压，&lt;/p&gt;

&lt;p&gt;只要调用足够频繁的话，也会累加出来一个较大的值，这会让报警的阈值非常难确定。&lt;/p&gt;

&lt;p&gt;在做2.0时我们尝试实现taskTopology的全自动伸缩，主要依赖的指标就是背压导致的自旋时间了。&lt;/p&gt;

&lt;p&gt;所以我一直在探索一个新的替代方案。&lt;/p&gt;

&lt;p&gt;最近有了一些突破，从原来关注Producer的计算密度，转向对consumer。原来ringbuffer选择的&lt;/p&gt;

&lt;p&gt;waitstrategy，由blockingtimeout改成了Lite。性能有很大的提升，减少了进入锁代码的的次数。&lt;/p&gt;

&lt;p&gt;在Lite策略的技术上，我增加了一个waittime的回调监听。每次wait之后出发notify。&lt;/p&gt;

&lt;p&gt;对这个waittime的累加，我们可以理解为下游消费者的繁忙程度，&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;离职&lt;/h2&gt;

&lt;p&gt;到这个月入职JD就满两年了，还是选择了在年前离开，这应该是我最后一次选择电商类的公司了。&lt;/p&gt;

&lt;p&gt;来年再战，感谢各位看官。&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Dec 2019 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2019-12-01-summary</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2019-12-01-summary</guid>
        
        
      </item>
    
  </channel>
</rss>
