<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pei LiPing's Blog</title>
    <description>Augur
</description>
    <link>http://peiliping.github.io/blog/</link>
    <atom:link href="http://peiliping.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 15 Jun 2017 15:15:44 +0800</pubDate>
    <lastBuildDate>Thu, 15 Jun 2017 15:15:44 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>谈谈日志解析中的正则表达式</title>
        <description>&lt;p&gt;正则表达式算是编程的基础知识，日常功能开发中也经常会用到。&lt;/p&gt;

&lt;p&gt;以Java为例，简单的一个String.format就会用到Pattern.match。&lt;/p&gt;

&lt;p&gt;Nginx的location配置，sed、grep等常用的linux命令也会用到正则。&lt;/p&gt;

&lt;p&gt;还有很多类似正则表达式的应用，比如Spring的PathURI解析。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;日志解析中的正则表达式&lt;/h2&gt;

&lt;p&gt;日志解析领域，是重度依赖Regex的。一提到正则就会有人说性能问题，耗Cpu。&lt;/p&gt;

&lt;p&gt;的确，由于正则的书写方式不合理，导致的性能问题非常之多，甚至还有死循环的bug。&lt;/p&gt;

&lt;p&gt;能写正则和写好正则之间需要大量的实践积累，也需要了解非常多的背景知识。&lt;/p&gt;

&lt;p&gt;去年做过一个日志采集工具，其中就用到了正则来解析NginxLog，性能调的还可以。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;正则表达式的几个基础概念&lt;/h3&gt;

&lt;p&gt;1、DFA和NFA引擎、回溯&lt;/p&gt;

&lt;p&gt;2、贪婪和非贪婪&lt;/p&gt;

&lt;p&gt;3、固化分组&lt;/p&gt;

&lt;p&gt;4、量词匹配、优先匹配&lt;/p&gt;

&lt;p&gt;具体概念就不展开讲了，自行查阅，提供２个blog可以看看。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/yangzhongxuan/article/details/6968556&quot;&gt;引擎&lt;/a&gt;、&lt;a href=&quot;http://blog.csdn.net/lxcnn/article/details/4756030&quot;&gt;贪婪与非贪婪&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;繁琐啰嗦的表达式&lt;/h3&gt;

&lt;p&gt;为了解析一行NginxLog，你可能会把正则写成这样:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;([%d]+/[%a]+/[%d]+:[%d]+:[%d]+:[%d]+ %+0800)&quot; ([%d|%.]+) (.-) ([%d|%.]+) ([%a|%-]+) ([%d]+) &quot;([http|https]+)://([^&quot;]*)&quot; ([%d]+) ([%d]+) &quot;([^&quot;]*)&quot; &quot;(.*)&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;精确的提供了每个字段的类型、长度等等，这样的表达式性能很好，但是看起来非常的啰嗦。&lt;/p&gt;

&lt;p&gt;有人为了省事，会大量用(.*)来替代表达式中的明确的类型和长度信息。&lt;/p&gt;

&lt;p&gt;简单测试一下就会发现(.*)的性能惨不忍睹。主要是因为大量的贪婪回溯，导致性能下降。&lt;/p&gt;

&lt;p&gt;注意：最后一个分组字段使用(.*)是非常合理的。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;简化一下&lt;/h3&gt;

&lt;p&gt;用(.*)来简化性能不好，那有没有性能好，又简单的表达式呢？可以试试非贪婪模式。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;(.-)&quot; (.-) (.-) (.-) (.-) (.-) &quot;(.-)://(.-)&quot; (.-) (.-) &quot;(.-)&quot; &quot;(.*)&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这个看起来是不是非常清晰，一个坑一个字段，最后一个分组仍然使用贪婪模式。&lt;/p&gt;

&lt;p&gt;看到这里是不是觉得以后正则表达式有可能自动生成了，不需要烧脑完成。&lt;/p&gt;

&lt;p&gt;这个正则的语法是以Lua为例的，其他语言也有类似的，只是符号不一样而已。Java是(.*?)&lt;/p&gt;

&lt;p&gt;这个表达式的性能比第一个啰嗦的表达式性能略差，差距很小，基本是可以接受的。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;继续深入&lt;/h3&gt;

&lt;p&gt;将简化的表达式进行一下整理替换，得到下面的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;([^&quot;]*)&quot; ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) &quot;([^:]*)://([^&quot;]*)&quot; ([^ ]*) ([^ ]*) &quot;([^&quot;]*)&quot; &quot;(.*)&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这里就看到比较重的分隔符含义了，一般来说我们的日志都有分隔符在的。&lt;/p&gt;

&lt;p&gt;但列与列之间的分隔符并不一定一致，比如NginxLog，不知道为什么Nginx要把Log默认写成那个样子。&lt;/p&gt;

&lt;p&gt;实际应用中，这种风格写法的Regex非常的实用。看过一些国内日志业务的解析功能也大概是这个思路吧。&lt;/p&gt;

&lt;p&gt;性能上比非贪婪的要好，和第一个表达式的性能是一样的。&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;自动化&lt;/h3&gt;

&lt;p&gt;上面两个正则表达式看起来规律性都非常强，也就是说有极大的可能可以自动生成这个表达式。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;$time_local&quot; $remote_addr $upstream_addr $request_time $request_method $status &quot;$scheme://$host$request_uri&quot; $request_length $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这个是Nginx的logformat配置，日志解析的正则表达式自动生成就很简单了。&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Jun 2017 13:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-06-15-regex</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-06-15-regex</guid>
        
        
        <category>regex</category>
        
      </item>
    
      <item>
        <title>Vertica优化之Encode</title>
        <description>&lt;p&gt;Vertica是一个成熟的商用列式数据库，也提供免费版本（限制数据规模小于1T，集群规模小于3台）。&lt;/p&gt;

&lt;p&gt;2015年尝试使用Vertica来存储时序类型的数据，主要应用在内部的JVM监控数据上。&lt;/p&gt;

&lt;p&gt;每天增量在8千万行，保留最近2个月左右的数据，按时间对数据进行分区，保留总量在50亿行左右。&lt;/p&gt;

&lt;p&gt;查询的SQL以sum avg count where group by order by这些关键词为主，简单的数值统计需求。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;时序数据的表有哪些字段&lt;/h3&gt;

&lt;p&gt;谈到时序数据就会想到监控，常见的开源项目有OpenTSDB、Graphite、Druid、Influxdb等等。&lt;/p&gt;

&lt;p&gt;年初Facebook开源的Beringer也掀起了一波浪潮，在规律的数据场景下，如何压缩存储数据呢？&lt;/p&gt;

&lt;p&gt;时序数据表中一般会有如下几种类型的列：&lt;/p&gt;

&lt;p&gt;1、时间戳，可能有不同精度的时间戳（ms、s、min、hour、day等）&lt;/p&gt;

&lt;p&gt;2、metricid、typeid，用来定义数据的类别&lt;/p&gt;

&lt;p&gt;3、dimensionid，数字化的维度信息，比如国家地区编码、性别、年龄、操作系统等等&lt;/p&gt;

&lt;p&gt;4、tag，跟维度类似，但是由于不方便定义成id，就以原始的字符串存下来了&lt;/p&gt;

&lt;p&gt;5、value，数值型用来存储最主要的信息&lt;/p&gt;

&lt;h3 id=&quot;verticaencode&quot;&gt;如何选择Vertica的Encode&lt;/h3&gt;

&lt;p&gt;1、经过测试时间戳类型的字段用COMMONDELTA_COMP存储空间占用最小，比BLOCK_DICT好很多。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;This compression scheme builds a dictionary of all deltas in the block and 
then stores indexes into the delta dictionary using entropy coding.
This scheme is ideal for sorted FLOAT and INTEGER-based
(DATE/TIME/TIMESTAMP/INTERVAL) data columns with predictable sequences and
only occasional sequence breaks, such as timestamps recorded at periodic 
intervals or primary keys. For example, the following sequence compresses 
well: 300, 600, 900,1200, 1500, 600, 1200, 1800, 2400. The following sequence 
does not compress well: 1,3, 6, 10, 15, 21, 28, 36, 45, 55.
If delta distribution is excellent, columns can be stored in less than one bit 
per row. However, this scheme is very CPU intensive. If you use this scheme on 
data with arbitrary deltas, it can cause significant data expansion.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2、MetricId和DimId之类的，一般来说value的变化比较少，比较适合BLOCK_DICT。&lt;/p&gt;

&lt;p&gt;3、tag类型字段选择了RLE（Run Length Encoding），效果还可以。&lt;/p&gt;

&lt;p&gt;4、Value的选择是最难的，前面提到的字段一般都是经过排序的，规律性很强。&lt;/p&gt;

&lt;p&gt;value的变化范围比较大，还有可能是小数，又没经过排序。经过一系列测试在性能和存储空间&lt;/p&gt;

&lt;p&gt;开销上权衡，最理想的encode方案是GCDDELTA&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;For INTEGER and DATE/TIME/TIMESTAMP/INTERVAL columns, and NUMERIC
columns with 18 or fewer digits, data is recorded as the difference from the 
smallest value in the data block divided by the greatest common divisor (GCD) of 
all entries in the block. This encoding has no effect on other data types.
ENCODING GCDDELTA is best used for many-valued, unsorted, integer columns or
integer-based columns, when the values are a multiple of a common factor. For
example, timestamps are stored internally in microseconds, so data that is only 
precise to the millisecond are all multiples of 1000. The CPU requirements for 
decoding GCDDELTA encoding are minimal, and the data never expands, but GCDDELTA 
may take more encoding time than DELTAVAL.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;sql&quot;&gt;建表SQL&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE PROJECTION public.jvm_metrics_tmp
(
 parent_id ENCODING BLOCK_DICT,
 metric_id ENCODING BLOCK_DICT,
 dim1 ENCODING BLOCK_DICT,
 dim2 ENCODING BLOCK_DICT,
 dim3 ENCODING BLOCK_DICT,
 dim4 ENCODING BLOCK_DICT,
 mtc1 ENCODING GCDDELTA,
 mtc2 ENCODING GCDDELTA,
 mtc3 ENCODING GCDDELTA,
 mtc4 ENCODING GCDDELTA,
 mtc5 ENCODING GCDDELTA,
 mtc6 ENCODING GCDDELTA,
 mtc7 ENCODING GCDDELTA,
 mtc8 ENCODING GCDDELTA,
 tag1 ENCODING RLE,
 tag2 ENCODING RLE,
 tag3 ENCODING RLE,
 tag4 ENCODING RLE,
 timestamps ENCODING COMMONDELTA_COMP,
 timestamps_1d ENCODING COMMONDELTA_COMP,
 timestamps_1h ENCODING COMMONDELTA_COMP,
 timestamps_10m ENCODING COMMONDELTA_COMP,
 timestamps_1m ENCODING COMMONDELTA_COMP,
 partition ENCODING COMMONDELTA_COMP
)
AS
 SELECT jvm_metrics.parent_id,
        jvm_metrics.metric_id,
        jvm_metrics.dim1,
        jvm_metrics.dim2,
        jvm_metrics.dim3,
        jvm_metrics.dim4,
        jvm_metrics.mtc1,
        jvm_metrics.mtc2,
        jvm_metrics.mtc3,
        jvm_metrics.mtc4,
        jvm_metrics.mtc5,
        jvm_metrics.mtc6,
        jvm_metrics.mtc7,
        jvm_metrics.mtc8,
        jvm_metrics.tag1,
        jvm_metrics.tag2,
        jvm_metrics.tag3,
        jvm_metrics.tag4,
        jvm_metrics.timestamps,
        jvm_metrics.timestamps_1d,
        jvm_metrics.timestamps_1h,
        jvm_metrics.timestamps_10m,
        jvm_metrics.timestamps_1m,
        jvm_metrics.partition
 FROM public.jvm_metrics
 ORDER BY jvm_metrics.dim1,
          jvm_metrics.parent_id,
          jvm_metrics.metric_id,
          jvm_metrics.dim2,
          jvm_metrics.dim3,
          jvm_metrics.dim4,
          jvm_metrics.tag1,
          jvm_metrics.tag2,
          jvm_metrics.tag3,
          jvm_metrics.tag4,
          jvm_metrics.timestamps
UNSEGMENTED ALL NODES;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 05 Jun 2017 16:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-06-05-verticaencoding</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-06-05-verticaencoding</guid>
        
        
        <category>vertica</category>
        
        <category>encode</category>
        
      </item>
    
      <item>
        <title>批量写入Mysql</title>
        <description>&lt;p&gt;Mysql是最常用的一种关系型数据库，随着各种ORM框架的演进，操作数据库也变得越来越简单。&lt;/p&gt;

&lt;p&gt;但是，当你碰到一些极端需求时，还是要回到JDBC这个层面上来操作。&lt;/p&gt;

&lt;p&gt;上篇博客介绍了Meepo这个数据迁移工具，其中就涉及到大量的Mysql写入操作。&lt;/p&gt;

&lt;p&gt;下面介绍一下，我在批量写Mysql时碰到的一些问题。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;真正的批量写&lt;/h3&gt;

&lt;p&gt;使用JDBC来完成批量Mysql写入的一般方法是:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;this.connection.setAutoCommit(false);

this.preparedStatement = this.connection.prepareStatement(this.sql);
for (int i = 0; i &amp;lt; this.schema.size(); i++) {
    this.preparedStatement.setObject(i + 1, data[i], this.schema.get(i));
}
this.preparedStatement.addBatch();

...N次

this.preparedStatement.executeBatch();
this.connection.commit();
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;当你写完这段代码，测试性能就会发现，与单条Insert相比，并没有什么提升。&lt;/p&gt;

&lt;p&gt;因为，JDBC实际上仍是以行为单位，发送和执行写入操作的，Batch是假的。&lt;/p&gt;

&lt;p&gt;当你为Datasource的Url添加了参数rewriteBatchedStatements=true，才会真正批量执行。&lt;/p&gt;

&lt;p&gt;具体原因可以自行搜索这个参数，网上有很多文章介绍，性能可以提升三五倍。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;选择好数据库的伴侣——数据源连接池&lt;/h3&gt;

&lt;p&gt;开源的数据源连接池非常多，比如C3P0、Druid、Proxool、BoneCP、HikariCP…&lt;/p&gt;

&lt;p&gt;比较常用的是阿里开源的Druid，综合表现不错。HikariCP也是不错的选择。&lt;/p&gt;

&lt;p&gt;HikariCP和Druid的作者在Github上有一段关于性能测试的争论也可以看看。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;缓冲区&lt;/h3&gt;

&lt;p&gt;涉及到批量操作，就意味着要和缓冲区打交道了，你可能会想到数组、队列、栈等等。&lt;/p&gt;

&lt;p&gt;一般来说批量写入的缓冲区有这么几个要求：&lt;/p&gt;

&lt;p&gt;1、容量空间可控，比如1024条，写满的时候可以阻塞住。&lt;/p&gt;

&lt;p&gt;2、有超时机制，不要无限期等待。&lt;/p&gt;

&lt;p&gt;3、性能好，最好是无锁的。&lt;/p&gt;

&lt;p&gt;Disruptor的Ringbuffer可以满足上面的这些需求，只是初次上手会有一些难度。&lt;/p&gt;

&lt;p&gt;Ringbuffer强制了数据对象的复用，可以减少JVM GC的次数，对性能的提升非常有帮助。&lt;/p&gt;

&lt;p&gt;Ringbuffer的具体使用方法这里就不介绍了，网上有很多文章和例子。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;线程数和批量大小&lt;/h3&gt;

&lt;p&gt;适当的加大并行度和批量大小，是可以提高消费速度的。&lt;/p&gt;

&lt;p&gt;根据测试批量写的线程数不要超过Mysql的Cpu核数+1。&lt;/p&gt;

&lt;p&gt;批量的条数由单行数据大小决定，一般1000条左右。&lt;/p&gt;

&lt;h3 id=&quot;mysql&quot;&gt;Mysql性能优化&lt;/h3&gt;

&lt;p&gt;Mysql性能优化，参考网上的文章，这里就不详细介绍了。&lt;/p&gt;

&lt;p&gt;大多是关闭一些log，提高一些buffer，改变filesync的方式等。&lt;/p&gt;

&lt;p&gt;我们是使用阿里云的RDS进行的测试，几乎不用什么调整，性能就很不错了。&lt;/p&gt;

&lt;p&gt;需要注意表上的索引个数，索引是很影响写入性能的，尽量保证索引精简。&lt;/p&gt;

&lt;h3 id=&quot;jdbc&quot;&gt;JDBC优化&lt;/h3&gt;

&lt;p&gt;经过前面的优化后，再使用JFR对你的程序做一下热点代码的分析，就会发现集中在JDBC的代码上了。&lt;/p&gt;

&lt;p&gt;仔细阅读MysqlConnector的代码就会发现，可以改造的地方还是挺多的。&lt;/p&gt;

&lt;p&gt;下面举几个例子：&lt;/p&gt;

&lt;p&gt;1、批量数据的暂存容器&lt;/p&gt;

&lt;p&gt;提交到Preparestatment中的数据，会暂时存到一个ArrayList中。&lt;/p&gt;

&lt;p&gt;ArrayList跟HashMap一样，随着数据被多次add，就会触发resize和array copy。&lt;/p&gt;

&lt;p&gt;通过Arrays类的copyOf方法对原数组进行拷贝，长度为原数组的1.5倍+1。&lt;/p&gt;

&lt;p&gt;在你初始化一个Preparestatment的时候，根据你的批量大小，对其进行capacity的初始化，会带来不小的性能提升。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;this.batchArgsField = StatementImpl.class.getDeclaredField(&quot;batchedArgs&quot;);
this.batchArgsField.setAccessible(true);
...
this.batchArgsField.set((StatementImpl) ((DruidPooledPreparedStatement) this.preparedStatement).getStatement(), new ArrayList&amp;lt;Object&amp;gt;(stepSize));

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2、日期格式化&lt;/p&gt;

&lt;p&gt;一般数据库表都会有一些日期类型的字段，比如创建时间、更新时间等。&lt;/p&gt;

&lt;p&gt;Java中的日期类型格式化处理性能一般，比如SimpleDateFormat就是一个关键点。&lt;/p&gt;

&lt;p&gt;代码com.mysql.jdbc.PreparedStatement中的tsdf，就是用来格式化日期类型的。&lt;/p&gt;

&lt;p&gt;假如你的表中日期类型字段比较多，可以考虑替换掉SimpleDateFormat的实现。&lt;/p&gt;

&lt;p&gt;Mysql中的日期类型格式是比较固定的，并不需要SimpleDateFormat那么复杂的实现，&lt;/p&gt;

&lt;p&gt;可以使用Calender的接口，通过字符串拼接的方式完成日期数据的格式化。&lt;/p&gt;

&lt;p&gt;性能也是会有一定的提升。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public class DateFormatter extends SimpleDateFormat {

    public DateFormatter(String pattern) {
        super(&quot;&quot;, Locale.US);
    }

    @Override public StringBuffer format(Date date, StringBuffer toAppendTo, FieldPosition pos) {
        super.calendar.setTime(date);
        toAppendTo.append(&quot;'&quot;);
        toAppendTo.append(super.calendar.get(1));
        toAppendTo.append(&quot;-&quot;);
        if (super.calendar.get(2) &amp;lt; 9) {
            toAppendTo.append(0);
        }
        toAppendTo.append(super.calendar.get(2) + 1);
        toAppendTo.append(&quot;-&quot;);
        if (super.calendar.get(5) &amp;lt; 10) {
            toAppendTo.append(0);
        }
        toAppendTo.append(super.calendar.get(5));
        toAppendTo.append(&quot; &quot;);
        if (super.calendar.get(11) &amp;lt; 10) {
            toAppendTo.append(0);
        }
        toAppendTo.append(super.calendar.get(11));
        toAppendTo.append(&quot;:&quot;);
        if (super.calendar.get(12) &amp;lt; 10) {
            toAppendTo.append(0);
        }
        toAppendTo.append(super.calendar.get(12));
        toAppendTo.append(&quot;:&quot;);
        if (super.calendar.get(13) &amp;lt; 10) {
            toAppendTo.append(0);
        }
        toAppendTo.append(super.calendar.get(13));
        return toAppendTo;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3、异常处理&lt;/p&gt;

&lt;p&gt;一个完善的数据处理流程，必然要考虑异常情况，比如网络闪断，数据库重启等等。&lt;/p&gt;

&lt;p&gt;如何在请求失败后重试呢？一般的做法就是记录下批量写入的每一个数据，异常时重新写入一遍。&lt;/p&gt;

&lt;p&gt;前面基于Ringbuffer做了数据对象的复用，为了处理异常再次引入重复对象是很难接受的。&lt;/p&gt;

&lt;p&gt;最简单的办法就是利用前面初始化好的batchArgs，实际上它里面就记录了你写入的每一行数据。&lt;/p&gt;

&lt;p&gt;当出现异常时，只要把它拷贝出来，写入新的Preparestatement中，就可以再次执行了。&lt;/p&gt;

&lt;p&gt;注意executeBatch一定不能丢掉。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;List&amp;lt;Object&amp;gt; batchParams = ((JDBC4PreparedStatement) ((DruidPooledPreparedStatement) this.preparedStatement).getStatement()).getBatchedArgs();
Connection tc = this.dataSource.getConnection();
tc.setAutoCommit(false);
PreparedStatement tp = tc.prepareStatement(this.sql);
StatementImpl target = (StatementImpl) ((DruidPooledPreparedStatement) tp).getStatement();
List&amp;lt;Object&amp;gt; newParams = new ArrayList&amp;lt;&amp;gt;(batchParams);
this.batchArgsField.set(target, newParams);
tp.executeBatch();
tc.commit();
tp.close();
tc.close();
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;sql&quot;&gt;SQL语法&lt;/h3&gt;

&lt;p&gt;批量写入的SQL语法也不止一种，比较常见的有：&lt;/p&gt;

&lt;p&gt;1、INSERT INTO xxxx (…) VALUES (…)&lt;/p&gt;

&lt;p&gt;2、INSERT IGNORE INTO xxxx (…) VALUES (…)&lt;/p&gt;

&lt;p&gt;3、REPLACE INTO xxxx (…) VALUES (…)&lt;/p&gt;

&lt;p&gt;当主键冲突的时候如何处理，可以根据情况选择合适的SQL模式。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;参数&lt;/h3&gt;

&lt;p&gt;最后附上我所使用的URL参数&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rewriteBatchedStatements=true
useUnicode=true
characterEncoding=UTF-8
useSSL=false
verifyServerCertificate=false
failOverReadOnly=false
autoReconnect=true
autoReconnectForPools=true
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;经过这些优化，Meepo可以每秒从Mysql中读取十几万的数据，并写入到一个无索引的新表中。&lt;/p&gt;

&lt;p&gt;8G的JVM，YGC每8-10秒发生一次，Mysql的Cpu会被压满。&lt;/p&gt;
</description>
        <pubDate>Thu, 25 May 2017 16:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-05-25-batch2mysql</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-05-25-batch2mysql</guid>
        
        
        <category>java</category>
        
        <category>mysql</category>
        
        <category>batch</category>
        
        <category>ringbuffer</category>
        
        <category>jdbc</category>
        
      </item>
    
      <item>
        <title>Meepo</title>
        <description>&lt;p&gt;Meepo最开始是为了Mysql到Mysql的数据迁移开发的，今年年初对其进行了重构。&lt;/p&gt;

&lt;p&gt;目前可以基本替代Sqoop完成每天凌晨将Mysql数据拷贝到HDFS上的需求，数据格式为Parquet+Snappy。&lt;/p&gt;

&lt;p&gt;并且Meepo还提供了AVSC文件，方便Hive更新表结构（TBLPROPERTIES (‘avro.schema.url’=’hdfs://onlinecluster/xxxxxxxxxx)）。&lt;/p&gt;

&lt;p&gt;Meepo的Mysql数据表迁移功能也优化了很多：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;支持基本字段类型的自动匹配转化&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;自动识别表结构和主键&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;支持同时使用N个Plugin&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;提供了ReplacePlugin，用于系统升级中常见的表字段值替换需求（查另外一张表，来决定替换成什么值）&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这次重构中，还引入了Disruptor的Ringbuffer，作为数据缓冲区。&lt;/p&gt;

&lt;p&gt;Ringbuffer强制了对象复用，大幅度减少了GC的频率，使得Meepo可以用更小的JVM来完成任务，性能也有一定的提升。&lt;/p&gt;

&lt;p&gt;在我们的测试中，Meepo的吞吐能力一直在每秒6W行以上，极限可以达到每秒15W。&lt;/p&gt;

&lt;p&gt;这次重构后的测试中，也找到了更多应该暴露的Metric指标，方便对Meepo进行调优。&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 09:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-04-18-meepo</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-04-18-meepo</guid>
        
        
        <category>java</category>
        
        <category>mysql</category>
        
        <category>sqoop</category>
        
        <category>parquet</category>
        
        <category>avsc</category>
        
        <category>ringbuffer</category>
        
      </item>
    
      <item>
        <title>Java Flight Recorder</title>
        <description>&lt;p&gt;Profiling是最常见的，用来定位代码性能问题的方法。&lt;/p&gt;

&lt;p&gt;在开发环境中，我们用Visualvm、JProfiler、Yourkit等。这些工具功能强大，支持图形化界面操作，可以让我们很快定位代码问题。&lt;/p&gt;

&lt;p&gt;但是他们对应用性能的影响也非常大，所以不适合在生产环境下使用。还有这些软件要attach到jvm进程上，生产环境一般网络隔离，很难做到。&lt;/p&gt;

&lt;p&gt;在生产环境我们最常用的profiling工具就是java/bin下的jstack，多做几次jstack，也相当于profiling了。有很多工具就是对多次的jstack结果进行合并，来分析问题的。&lt;/p&gt;

&lt;p&gt;jstack方便易用，但并不是特别适合来做profiling，操作频率低，会导致safepoint指标急剧增长等等。&lt;/p&gt;

&lt;p&gt;于是我们尝试使用Jvm原生提供的JFR - Java Flight Recorder 来解决问题。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;使用方法&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;jcmd pid VM.unlock_commercial_features&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;jcmd pid JFR.start duration=60s filename=/home/peiliping/dev/logs/test.jfr settings=[default,profile]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;jcmd pid JFR.check 检查当前运行状况&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;jcmd pid JFR.stop name=test.jfr 或者 jcmd pid JFR.stop recording=3 name和recording的值参见check的结果&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将生成的jfr文件传回到自己的电脑中&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;shell下执行jmc，启动图形化客户端，导入jfr文件，就可以看到cpu、gc、线程、io、代码热点等监控信息了&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果需要定制化jfr抓取的指标，可以修改setting的xml文件，/…./JDK/jdk1.8.0_51/jre/lib/jfr/&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 31 Mar 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-03-31-jfr</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-03-31-jfr</guid>
        
        
        <category>java</category>
        
      </item>
    
      <item>
        <title>Nginx配置</title>
        <description>&lt;p&gt;今年进行了一次Nginx的大面积升级，把Nginx的安装配置方法记录一下。&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;依赖准备&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;下载一个稳定版本的&lt;a href=&quot;https://nginx.org/en/download.html&quot;&gt;Nginx源码包&lt;/a&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;因为Nginx一般都是挡在最前面的系统，所以要做一些基本的保护，防止死机，github上有一些开源项目可以集成到Nginx中去，比如nginx-http-sysguard。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NginxPlus版本有自定义心跳检查的功能，但是免费版没有提供。为什么要自定义，而不是用默认的upstream心跳检查呢？如果能自定义的话，可以做更加优雅平滑的发布，还可以做引流压测。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nginx默认的监控比较弱，同样丰富的监控都在NginxPlus里提供，只能自己开发了。配套定制一下Tsar，将监控信息记录在Tsar中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;现在绝大多数网站都是https了，所以openssl和证书也要准备好。openssl这两年出了好多头条，慎重选择。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nginx编译时依赖的一些包，比如：pcre、zlib、perl-devel、perl-ExtUtils-Embed等&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nginx的服务器一般都有外网IP，所以也需要定制一些防火墙策略。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;为了提高Nginx的吞吐能力需要修改一些系统参数（/etc/sysctl.conf和/etc/security/limits.conf）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nginx日志默认是没有daily-rolling功能的，需要配和其他软件来实现。之前是用cronlog和fifo文件来实现的，这次统一换成logrotate，logrotate相关资料这里就不介绍了。注意不要使用copy模式，Nginx的accesslog很大，copy非常消耗性能。下面给一个配置的例子：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/log/nginx/*.log {
    daily
    rotate 1
    missingok
    nocompress
    notifempty
    dateext
    postrotate
    if [ -f /pid/nginx.pid ]; then
        kill -USR1 `cat /pid/nginx.pid`
    fi
    endscript
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;section-1&quot;&gt;编译安装&lt;/h3&gt;

&lt;p&gt;Nginx启动一般需要80端口，如果不想用root来启动的话，就需要特殊处理一下&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;src/nginx/configure --prefix=/local/nginx ....
make -j ${nc} &amp;amp;&amp;amp; make install
chown -R nginx:nginx /local/nginx
chown root:nginx /local/nginx/sbin/nginx
chmod 6755 /local/nginx/sbin/nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;nginx&quot;&gt;Nginx配置&lt;/h3&gt;

&lt;p&gt;Nginx的配置实在是太多了，下面只写一些值得注意的配置项&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;worker_processes和worker_cpu_affinity 在新版nginx中都可以设置为auto&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;log_format中$upstream_addr最好用引号或者中括号扩起来，因为他的输出不仅仅是一个IP，可能是多个Ip或者upstream的名字或者是中横线&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;merge_slashes合并url中连续出现的两个/，防止url拼接错误导致无法访问&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;proxy_set_header设置要注意，在http、server、location层都能够进行定义，但是并不是merge覆盖的逻辑，官方文档是这样说的，These directives are inherited from the previous level if and only if there are no proxy_set_header directives defined on the current level。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;proxy_ignore_client_abort默认为off，千万不要轻易开启。可能会导致Nginx活跃链接数持续增长，tcp状态close_wait持续增长，只能重启Nginx进程才能回收，网上说的修改linux内核参数也无法回收。大致过程是，客户端发起请求，后端的业务服务器无法及时响应该请求等待超时，如果客户端配置了较短的超时，客户端就会主动关闭此连接，当Nginx断开与tomcat的proxy连接时，客户端连接早已不存在了，此连接就会挂在Nginx上，无法释放。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ssl_ciphers设置不当可能导致某些浏览器无法访问，认为网站是不安全的，在IE和Firefox中尤为明显。ssl相关配置有一些&lt;a href=&quot;https://mozilla.github.io/server-side-tls/ssl-config-generator/&quot;&gt;自动生成器&lt;/a&gt;，github上也有一些自动的shell脚本可以参考。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;简化proxy_pass对应upstream的配置，可以将upstream的名字设置为host的值(proxy_pass http://$host;)，这样很多server的配置就可以复用了。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;server同时有80和443可以配置成这样：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;listen 80;
listen 443 ssl;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</description>
        <pubDate>Fri, 16 Dec 2016 14:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2016-12-16-nginx</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2016-12-16-nginx</guid>
        
        
        <category>nginx</category>
        
      </item>
    
      <item>
        <title>日志采集工具Logwatch</title>
        <description>&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2016年11月到12月中旬，开发了&lt;a href=&quot;https://github.com/peiliping/logwatch&quot;&gt;Logwatch&lt;/a&gt;的第一个版本。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;去年公司生产环境下的应用日志采集，主要用的是Logstash，也就是ELK豪华套餐中的“L”。碰到的主要问题是：Cpu开销高、内存占用高（比如采集NginxAccessLog，NgxQPS 1000+ ，Logstash要消耗2G的内存，Cpu开销也有很大的波动）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;市面上与Logstash同类的开源项目有Flume、Heka、Fluentd、Graylog、Logkafka，以及各种Syslog和Collecter。不仅开源世界百花齐放，几乎每家云计算服务提供商还都提供日志采集、传输和搜索服务。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;简单总结一下这些开源项目或多或少存在的一些问题：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;只支持单行Log，无法对Java应用中的ExceptionLog进行有效的采集和解析&lt;/li&gt;
    &lt;li&gt;解析日志不够灵活&lt;/li&gt;
    &lt;li&gt;性能一般&lt;/li&gt;
    &lt;li&gt;不支持Kafka作为输出&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;自己造一个轮子要达到什么目标呢？&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;代码要少，简单可控&lt;/li&gt;
    &lt;li&gt;性能要好，不能因为采集日志影响了应用的性能&lt;/li&gt;
    &lt;li&gt;支持新版Kafka(0.10+)&lt;/li&gt;
    &lt;li&gt;支持单双行混合模式的应用日志&lt;/li&gt;
    &lt;li&gt;对简单格式的日志可以不用编写正则表达式，根据类似nginx的logformat配置格式，自动生成解析正则,也就是logwatch里的grok功能&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Lua程序优化的心得&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Lua的协程切换开销非常低，封装调度任务非常合适&lt;/li&gt;
    &lt;li&gt;Lua的正则性能非常好，其非贪婪匹配‘(.-)’，可以应用于非常多的解析场景，简单高效,控制好回溯，尽量明确字符类型&lt;/li&gt;
    &lt;li&gt;table要尽量复用，减少创建table和table.insert的过程中resize的开销&lt;/li&gt;
    &lt;li&gt;不要反复做字符串拼接，必要时用table.concat替代&lt;/li&gt;
    &lt;li&gt;尽量使用ipairs，少使用table.foreach&lt;/li&gt;
    &lt;li&gt;尽量不要相信网上传说的Lua优化写法，Luajit已经优化了绝大多数问题了&lt;/li&gt;
    &lt;li&gt;Luajit有profile功能，可以用于测试代码热点&lt;/li&gt;
    &lt;li&gt;程序运行一次很快，运行几亿次再看看性能如何&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Thu, 15 Dec 2016 20:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2016-12-15-logwatch</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2016-12-15-logwatch</guid>
        
        
        <category>lua</category>
        
        <category>log</category>
        
        <category>kafka</category>
        
      </item>
    
  </channel>
</rss>
