<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pei LiPing's Blog</title>
    <description>Augur
</description>
    <link>http://peiliping.github.io/blog/</link>
    <atom:link href="http://peiliping.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 24 Feb 2018 16:52:54 +0800</pubDate>
    <lastBuildDate>Sat, 24 Feb 2018 16:52:54 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>类型转换</title>
        <description>&lt;p&gt;之前几个月一直在忙一个binlog抽取的项目，将mysqlbinlog拉出来，写入kafka，&lt;/p&gt;

&lt;p&gt;之后消费kafka中的数据写到HDFS，文件格式为orcfile+snappy。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;背景&lt;/h2&gt;

&lt;p&gt;上一篇blog讲过kafka中的数据是以avro为载体的，其中的数据字段存在一个map中，&lt;/p&gt;

&lt;p&gt;map的key和value都是charsequence，也就是说我们的数据在经过avro之后失去了类型信息。&lt;/p&gt;

&lt;p&gt;我们的目标端是写入hdfs上的orcFile，如果是写sequencefile，基本上我们就不关心类型了。&lt;/p&gt;

&lt;p&gt;阿里早期的数据仓库中，几乎所有的字段类型都是string的，这样做显然会有空间的浪费，&lt;/p&gt;

&lt;p&gt;但也比较方便，不容易出错，方便管理。在前一家公司里，我负责的离线数据都是以parquet为主的，&lt;/p&gt;

&lt;p&gt;orc相比parquet查询的性能会更快一些。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;类型转换&lt;/h2&gt;

&lt;p&gt;orc是有字段类型概念的，那么我们如何将string转成具体的类型呢？&lt;/p&gt;

&lt;p&gt;首先我们要知道原始类型（mysql中的类型），还要知道hive表中的字段类型（orc类型）。&lt;/p&gt;

&lt;p&gt;知道了这两端的类型，我们就有可能完成这个工作了。最简单的处理办法就是笛卡尔积。&lt;/p&gt;

&lt;p&gt;把每种组合的处理函数写好，进行配置就可以了。这样做的缺点非常明显，就是工作量大。&lt;/p&gt;

&lt;p&gt;如果有一天我们不再使用orc格式的类型，换成parquet或者其他的，那么还需要大量的重复工作。&lt;/p&gt;

&lt;p&gt;于是解决这个问题的关键是降低耦合度，降低复杂度。&lt;/p&gt;

&lt;p&gt;之前在阅读阿里开源的datax的时候看到过一个类似问题的解决方案，引入状态机。&lt;/p&gt;

&lt;p&gt;mysql的常见字段类型大概不到20种，为每种类型创建一个type类，并提供转成其他类型的方法。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public interface JavaType {

    Boolean toBoolean(String value);

    Integer toInt(String value);

    Long toLong(String value);

    Date toDate(String value);

    Float toFloat(String value);

    Double toDouble(String value);

    default String toString(String value) {
        return value;
    }

    default void unsupport() {
        throw new RuntimeException(&quot;type error&quot;);
    }

}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;碰到把boolean转date显然是不可能实现的，那么就直接unsupport好了。&lt;/p&gt;

&lt;p&gt;接下来解决orc端的问题&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; TINYINT(new C_LongColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createByte();
        }
    }, SMALLINT(new C_LongColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createShort();
        }
    }, INT(new C_LongColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createInt();
        }
    }, BIGINT(new C_LongColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createLong();
        }
    }, BOOLEAN(new C_BooleanColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createBoolean();
        }
    }, FLOAT(new C_DoubleColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createFloat();
        }
    }, DOUBLE(new C_DoubleColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createDouble();
        }
    }, DECIMAL(new C_DecimalColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createDecimal();
        }
    }, STRING(new C_BytesColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createString();
        }
    }, BINARY(new C_BytesColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createBinary();
        }
    }, CHAR(new C_BytesColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createChar();
        }
    }, VARCHAR(new C_BytesColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createVarchar();
        }
    }, TIMESTAMP(new C_TimeStampColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createTimestamp();
        }
    }, DATE(new C_DateColumnVector()) {
        @Override
        public TypeDescription toOrcTypeDescption() {
            return TypeDescription.createDate();
        }
    };

    private Convert convert;

    OrcTypeEnum(Convert convert) {
        this.convert = convert;
    }

    public static OrcTypeEnum findType(String type) {
        return OrcTypeEnum.valueOf(type.toUpperCase());
    }

    public abstract TypeDescription toOrcTypeDescption();

    public void setValue(ColumnVector vector, int row, String value, JavaType typeConvert) {
        convert.eval(vector, row, value, typeConvert);
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;为orc的每种类型创建一个convert，大概十来种的样子。&lt;/p&gt;

&lt;p&gt;通过这种方式，我们不仅解决了正常的类型转化需求，还能够天然支持date to long 和long to date这样的复杂需求，&lt;/p&gt;

&lt;p&gt;极大的提高了工具的灵活度。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;函数化&lt;/h2&gt;

&lt;p&gt;之前datax的方案是引入一个实体类对原始类型的数据进行包装，如果每一个字段都经过一次包装会严重增加体积，&lt;/p&gt;

&lt;p&gt;ygc的频率会提高，所以在我们的方案中是函数化的，相关类型进行编号，性能也有一些提升。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    public void writeData(List&amp;lt;String&amp;gt; result, List&amp;lt;ColumnInfo&amp;gt; columnInfos, List&amp;lt;JavaType&amp;gt; javaTypeList) {
        int row = this.batch.size++;
        for (int i = 0; i &amp;lt; result.size(); i++) {
            ColumnInfo col = columnInfos.get(i);
            col.getHiveTypeEnum().setValue(batch.cols[i], row, result.get(i), javaTypeList.get(col.getJavaTypeFunctionId()));
        }
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</description>
        <pubDate>Sat, 20 Jan 2018 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2018-01-20-typeconvert</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2018-01-20-typeconvert</guid>
        
        
        <category>type</category>
        
        <category>convert</category>
        
        <category>parser</category>
        
      </item>
    
      <item>
        <title>关于Avro的使用</title>
        <description>&lt;p&gt;14年开始尝试Flume的时候了解到Avro相关的东西，但一直都没有很深入的使用。&lt;/p&gt;

&lt;p&gt;最近在做Binlog采集时，写入Kafka的数据格式要求是Avro，在使用中了解碰到了一些小问题&lt;/p&gt;

&lt;p&gt;在这里分享一下。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;复用&lt;/h2&gt;

&lt;p&gt;序列化和反序列化时需要BinaryEncoder和BinaryDecoder，这个对象是可以反复使用的。&lt;/p&gt;

&lt;p&gt;反序列化接口是支持对象复用的，但是经过测试复用对象性能反而下降了。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;并发&lt;/h2&gt;

&lt;p&gt;序列化和反序列化都需要一个Json格式的Schema信息，或者叫Avsc。&lt;/p&gt;

&lt;p&gt;最开始多线程反序列化Avro数据时，是使用同一个Schema对象的，但是随着线程的增加，&lt;/p&gt;

&lt;p&gt;并发的效率提升并不明显，通过对线程状态的搜索，发现Schema对象上有Sync修饰，&lt;/p&gt;

&lt;p&gt;详见JsonProperties类。解决办法也很简单，每个线程使用一个独立的Schema对象就可以了。&lt;/p&gt;

&lt;h2 id=&quot;avrostring&quot;&gt;Avro中的String&lt;/h2&gt;

&lt;p&gt;Avro中的String可以是Java的String类型，还可以是使实现了CharSequence的UTF8。&lt;/p&gt;

&lt;p&gt;当然UTF8的Charsequence直接tostring就会转为Java的String了，但是Map的key就非常难处理。&lt;/p&gt;

&lt;p&gt;假如你的数据对象中有一个Map的field，并且Map的Key是String，通过get(“abc”)来获取&lt;/p&gt;

&lt;p&gt;value是不可行的，因为”abc”和UTF8(“abc”)的hashcode完全不同。只能遍历老的map，然后&lt;/p&gt;

&lt;p&gt;key.tostring()，再放入一个新的map中去，非常浪费性能。&lt;/p&gt;

&lt;p&gt;Avro是可以声明schema的string是java的string的来解决，还可以复写SpecificDatumReader&lt;/p&gt;

&lt;p&gt;这个类，特殊处理一下readMapKey方法，当是UTF8时调用一下tostring方法，转成java的string。&lt;/p&gt;
</description>
        <pubDate>Wed, 29 Nov 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-12-25-avro</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-12-25-avro</guid>
        
        
        <category>avro</category>
        
      </item>
    
      <item>
        <title>关于Mysql的Binlog抽取</title>
        <description>&lt;p&gt;这个月换了工作，双十一之前入职了京东大数据平台——实时数据组。&lt;/p&gt;

&lt;p&gt;着手关于MysqlBinlog相关系统的重构工作，这次就讲讲Binlog吧。&lt;/p&gt;

&lt;h2 id=&quot;binlog&quot;&gt;Binlog转实时数据流的开源动态&lt;/h2&gt;

&lt;p&gt;伪装MysqlSlave来获取MysqlBinlog的技术这几年已经非常普及了，很多中等规模的公司都有相关的&lt;/p&gt;

&lt;p&gt;基础技术组件，大多数是基于阿里几年前开源的Canal项目，在此基础上进行二次开发，将数据写入Kafka&lt;/p&gt;

&lt;p&gt;或者其他MQ系统中去，阿里云的RDS还提供类似的服务，也非常省心。&lt;/p&gt;

&lt;p&gt;17年年初的时候我也有意要做类似的项目，所以已经进行了最基础的技术选型和调研。&lt;/p&gt;

&lt;p&gt;Java系中最早的binlog开源项目有tungsten-replicator 、open-replicator等，后来都慢慢废弃了。&lt;/p&gt;

&lt;p&gt;之后Canal几乎统一了这个领域，但是阿里的这个开源项目最近几年的更新频率很一般。&lt;/p&gt;

&lt;p&gt;最近两年比较新的项目有mysql-time-machine，zendesk的maxwell都不错。&lt;/p&gt;

&lt;p&gt;shyiko的mysql-binlog-connector-java是一个BinlogClient的超精简内核，也被很多开源项目采用。&lt;/p&gt;

&lt;h2 id=&quot;mysqlbinlog&quot;&gt;Mysql的Binlog&lt;/h2&gt;

&lt;p&gt;Mysql的Binlog协议和格式我就不多介绍了，网上可以搜到很多。&lt;/p&gt;

&lt;p&gt;值得注意的时候，Mysql一定要配置成Row模式的，而且image需要配置成Full模式的。&lt;/p&gt;

&lt;p&gt;我采用shyiko的mysql-binlog-connector-java的方案进行了一下性能测试，可以达到80M/s。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;数据解析&lt;/h2&gt;

&lt;p&gt;shyiko的mysql-binlog-connector-java已经将数据解析成Java类型了，接下来只需要针对公司的规范&lt;/p&gt;

&lt;p&gt;进行数据的清洗和整理即可，最后转成标准序列化格式，比如AVRO、Pb、Json等。&lt;/p&gt;

&lt;p&gt;这中间性能损耗最大的地方就是序列化的开销了，要考虑异步或者多线程流等方式解决。&lt;/p&gt;

&lt;p&gt;在开发解析逻辑时碰到了不少细节问题，用shyiko的mysql-binlog-connector-java解析的数据与canal&lt;/p&gt;

&lt;p&gt;有一些细小的差别，比如datetime和timestamp字段解析的结果不一样，有可能受时区的影响；&lt;/p&gt;

&lt;p&gt;还有像text类型的字段是byte数组，需要自己指定charset转为string；decimal也有差异，需要使用&lt;/p&gt;

&lt;p&gt;numberformat进行格式化，group设置为false，并且setMinimumFractionDigits(1)。&lt;/p&gt;

&lt;h2 id=&quot;kafka&quot;&gt;写Kafka有序&lt;/h2&gt;

&lt;p&gt;将数据写到Kafka的速度是非常快的，但是因为Binlog的特殊性，需要一些设计，来保证数据的有序。&lt;/p&gt;

&lt;p&gt;Kafka的Client会把收到的数据根据meta进行进行重新组织，按照broker的使用情况进行分批发送。&lt;/p&gt;

&lt;p&gt;所以我们要根据业务情况指定Kafka的MessageKey，将相同业务含义的数据写到同一个Partition&lt;/p&gt;

&lt;p&gt;下，保证有序，乱序会导致最新值被老值覆盖。&lt;/p&gt;

&lt;p&gt;在写Kafka的过程中难免会有一些失败的情况发生，错误的重试机制由Kafka来保证，并且要强制设置&lt;/p&gt;

&lt;p&gt;ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION = 1，以防止重试时发生数据顺序错误。&lt;/p&gt;

&lt;h2 id=&quot;task&quot;&gt;Task的结构&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;P1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;B1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;P2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;B2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;P3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;单线程拉Binlog&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Ringbuffer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;单线程转化成AVRO对象&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Ringbuffer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;序列化并发送Kafka&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Ringbuffer&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&amp;gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;序列化并发送Kafka&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;注释：P一般对应一个线程，B对应一个Disrupter的Ringbuffer。&lt;/p&gt;

&lt;p&gt;P1和P2环节的性能都非常好，所以采用的是单线程，阶段并行模式。&lt;/p&gt;

&lt;p&gt;P3阶段采用并行模式，P2处理好的数据根据一些key进行选择，路由到N个B2中去，一般可以用库表名称。&lt;/p&gt;

&lt;p&gt;压测时，这个Task跑起来之后可以让负载达到400%，基本上把Cpu资源跑满了。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;持久化位点信息&lt;/h2&gt;

&lt;p&gt;什么是位点信息？&lt;/p&gt;

&lt;p&gt;简单来说就是binlog的filename和offset，为了保证at least once，&lt;/p&gt;

&lt;p&gt;我们需要定期记录消费的位置，以便任务重启之后，继续消费，不丢数据（可有少量的重复）。&lt;/p&gt;

&lt;p&gt;如果P3阶段是单线程的，那么记录位点非常简单，提交kafka成功后，记录最后一条数据的offset，&lt;/p&gt;

&lt;p&gt;可以异步刷到远程存储中去，防止阻塞数据流。可是，现在P3阶段是多线程的，如何记录位点呢？&lt;/p&gt;

&lt;p&gt;定期让P2对所有的B2发送一条相同的含有位点信息的心跳包（位点值为P2最后处理的一条数据的位点）&lt;/p&gt;

&lt;p&gt;通过这种方式，将各个P3的信息进行更新，每次P3将数据成功发送到Kafka后就将其中的心跳包记录下来，&lt;/p&gt;

&lt;p&gt;作为一个可信的回退点，注意这个心跳包一定要在数据流中流转，起到挤压数据的作用，&lt;/p&gt;

&lt;p&gt;防止某些P3无数据的情况发生。相关代码参考我的sucker项目，这里就不介绍代码细节了。&lt;/p&gt;

&lt;p&gt;解决这个问题的思路源于Flink介绍文章中的一段话：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stream Barrier是Flink分布式Snapshotting中的核心元素，它会作为数据流的记录被同等看待，

被插入到数据流中，将数据流中记录的进行分组，并沿着数据流的方向向前推进。每个Barrier会携带

一个Snapshot ID，属于该Snapshot的记录会被推向该Barrier的前方。因为Barrier非常轻量，所以

并不会中断数据流。带有Barrier的数据流。

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 29 Nov 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-11-29-mysqlbinlog</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-11-29-mysqlbinlog</guid>
        
        
        <category>mysql</category>
        
        <category>binlog</category>
        
      </item>
    
      <item>
        <title>如何设计日活的流计算</title>
        <description>&lt;p&gt;之前在Redis上构建了基于Bitmap索引的存储结构，在Openresty上实现了一个简易的查询引擎，&lt;/p&gt;

&lt;p&gt;这里再思考一下数据的来源、采集、实时接入等问题。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;日活数据的来源&lt;/h2&gt;

&lt;p&gt;日活数据最大的可能来源就是浏览日志、登录日志等，数据的内容一般包括时间戳、UID、PageId等，&lt;/p&gt;

&lt;p&gt;一般网站或者App统计日活是需要区分模块和功能的，类似友盟、TalkingData等公司的产品，以一个&lt;/p&gt;

&lt;p&gt;SDK或者Agent的方式采集和发送数据。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;数据接收&lt;/h2&gt;

&lt;p&gt;客户端采集到的数据会定期发送回来，接收端并没有复杂的业务处理逻辑，基本上就是format。&lt;/p&gt;

&lt;p&gt;当接收端收到数据做了基本上的合法性校验之后，可以选择写日志或者发送到队列中（比如Kafka）。&lt;/p&gt;

&lt;p&gt;Openresty加Lua可以完成这部分的工作，性能会非常强悍，我比较倾向直接写本地Log，&lt;/p&gt;

&lt;p&gt;然后批量发送至Kafka。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;关于维度信息的计算&lt;/h2&gt;

&lt;p&gt;因为我们是要做维度数据统计的，所以在数据处理过程中就需要为访问日志数据添加维度的信息。&lt;/p&gt;

&lt;p&gt;维度信息大体可以分为两大类，一类是固定维度，一类是变动维度。固定维度就类似性别、年龄等，&lt;/p&gt;

&lt;p&gt;变动维度可以是所在的地理位置等信息。维度的数据经常是需要进行编码的，就是将string转id。&lt;/p&gt;

&lt;p&gt;客户端每次上传的数据中是冗余维度信息，还是在流计算中去获取转化丰富数据的维度，这个逻辑&lt;/p&gt;

&lt;p&gt;通常就是getOrCreate，如果是有一个Mysql的表来存储这种数据，再加一个Redis的Cache，这种&lt;/p&gt;

&lt;p&gt;中心化的设计在数据流量大的时候必然是瓶颈，所以要适当考虑在客户端做一些工作。比如，在客户&lt;/p&gt;

&lt;p&gt;端启动时，进行一些基本维度的转化工作，并且将这些转化的数据进行保存，在定期提交数据时，&lt;/p&gt;

&lt;p&gt;主动将数据的string转id的工作在客户端完成。当然这样的分散逻辑也导致了维护成本，当你需要进行&lt;/p&gt;

&lt;p&gt;一些数据的整理和升级时，就会要跟客户端打交道了，需要设计一下数据的Version和避免过于集中的&lt;/p&gt;

&lt;p&gt;数据更新导致后端压力过大。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;关于时间窗口与聚合&lt;/h2&gt;

&lt;p&gt;无论你选择了哪一种流计算框架，都会提供一个Timewindow的功能，提供基本的Map、Reduce功能。&lt;/p&gt;

&lt;p&gt;如果我们的浏览访问数据存在极大的聚合利益，可以压缩几倍或者十几倍，就值得一做。&lt;/p&gt;

&lt;p&gt;如果考虑到数据的修复和数据Delay等问题，一定要设计一个增量补全的逻辑，比如有put和incr操作。&lt;/p&gt;

&lt;p&gt;之前看过一些文章有在Hbase上加协处理器来完成的，当然我们的Redis+Lua也是可以的。&lt;/p&gt;

&lt;p&gt;数据的输出结果尽量不要直接写存储，可以再打回Kafka中，做多订阅的处理。Kafka的客户端可以把&lt;/p&gt;

&lt;p&gt;单条输出转化为批量，这种设计比较容易规避流计算的一些迭代输出单条的麻烦。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;维度数据的存储&lt;/h2&gt;

&lt;p&gt;每个用户的每次访问都带有20个维度信息，这种沉重的冗余信息会让你的存储不堪重负的。&lt;/p&gt;

&lt;p&gt;因为我们的目标是日活，也就是以用户为主体。当某个用户已经被处理过一次了，那么他的固定维度&lt;/p&gt;

&lt;p&gt;数据最好就不用再处理了，每次只处理变动信息就好了，表的行数相对比较稳定，跟总用户数有关。&lt;/p&gt;

&lt;p&gt;如果只处理日活相对比较简单，如果想要按照时间做区分，比如每小时一个列，相当于行转列。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;UID&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Gender&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Age&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;00:00&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;01:00&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;02:00&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;03:00&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;F&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;M&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;F&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;M&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section-5&quot;&gt;水平扩展&lt;/h2&gt;

&lt;p&gt;如果单机Redis不能存下所有的数据，就需要有一个分布式的方案，因为我们的计算是列级别的，&lt;/p&gt;

&lt;p&gt;所以同一个Uid的所有列必须在一台Redis实例中，于是我们的分布式方案只能按照Uid来分了。&lt;/p&gt;

&lt;p&gt;因为我们的UID是使用16K、64K的Block块来划分的，所以我们只要对Block块进行离散就好了。&lt;/p&gt;

&lt;p&gt;计算引擎的改动也会很小，这里就是一个简单的MapReduce的过程。&lt;/p&gt;
</description>
        <pubDate>Sun, 08 Oct 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-10-08-redis-bit3</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-10-08-redis-bit3</guid>
        
        
        <category>redis</category>
        
        <category>lua</category>
        
        <category>bitmap</category>
        
        <category>lpeg</category>
        
      </item>
    
      <item>
        <title>如何查询Redis中的Bitmap数据</title>
        <description>&lt;p&gt;书接上回，我们已经将日活的维度数据写入Redis的Bitmap结构中。&lt;/p&gt;

&lt;p&gt;今天再设计一个简单的查询引擎，通过表达式转化为Redis的指令集，最终得到日活的结果。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;设计一个简单的表达式&lt;/h2&gt;

&lt;p&gt;最通用的表达式就是SQL了，Hive、Drill、Presto、Phoenix等开源项目都在做类似的事情。&lt;/p&gt;

&lt;p&gt;我们这个筛选维度求日活的功能，没有那么复杂，所以简单一些就好。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(C1 = 3 &amp;amp; C2 = 1) &amp;amp; (C5 = 1 | C8 = 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;其中C(N)代表选择第几个维度，等号后面代表维度的值，&lt;/p&gt;

&lt;p&gt;“&amp;amp;”代表AND操作，“|”代表OR操作，括号表示结合顺序。&lt;/p&gt;

&lt;p&gt;这个表达式的含义我想很容易理解，跟四则混合运算差不多。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;表达式的解释器&lt;/h2&gt;

&lt;p&gt;下面看看我们如何解析这个表达式，将表达式转化为Redis执行的指令。&lt;/p&gt;

&lt;p&gt;之前玩了一段时间的Lpeg，也做过一些小东西，这里还是用Lpeg来做，&lt;/p&gt;

&lt;p&gt;在四则混合运算的例子上加以改进即可。&lt;/p&gt;

&lt;p&gt;下面是对表达式的定义。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function Blk(p)
  return p * V &quot;Space&quot;
end

local G = P{
    V &quot;Space&quot; * V &quot;Stmt&quot; ;
    Stmt      = Cf(V &quot;Group&quot; * Cg(V &quot;LogicSig&quot; * V &quot;Group&quot;)^0 , operation) ,
    Group     = V &quot;Element&quot; + V &quot;Open&quot; * V &quot;Stmt&quot; * V &quot;Close&quot; ,
    Element   = Cg(Blk(V &quot;ColNum&quot;) * Blk(V &quot;EqSignal&quot;) * Blk(V &quot;ColVal&quot;) / selector) ,
    LogicSig  = Blk(C(S &quot;&amp;amp;|&quot;)),

    ColNum    = P &quot;C&quot; * C(R &quot;09&quot;^1) ,
    EqSignal  = C(P &quot;=&quot;) ,
    ColVal    = C((R &quot;az&quot; + R &quot;AZ&quot; + R &quot;09&quot;)^1) ,

    Open      = Blk(P &quot;(&quot;) ,
    Close     = Blk(P &quot;)&quot;) ,
    Space     = S(&quot; \n\t&quot;)^0 ,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在解析到 C1=1 类似的结构时用selector函数处理&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function selector(colNum , op , colVal)
  lastKey = {&quot;b&quot; , colNum .. '-' .. colVal}
  return lastKey
end
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;解析到 XXX &amp;amp; YYY 或者 XXX | YYY 的结构时用operation函数处理&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function operation(left , op , right)
  local action = &quot;&quot;
  if op == &quot;&amp;amp;&quot; then 
    action = &quot;AND&quot;
  elseif op == &quot;|&quot; then
    action = &quot;OR&quot;
  end
  local tmpKey = &quot;__TMP-&quot; .. tmpCount
  lastKey = {&quot;t&quot; , tmpKey}
  table.insert(recycle , tmpKey)
  tmpCount = tmpCount + 1
  table.insert(process , {&quot;BITOP&quot; , action , tmpKey , left[1] , left[2] , right[1] , right[2]})
  return {&quot;t&quot; , tmpKey }
end
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;因为在Bitmap运算过程中，我们会产生一些临时的Bitmap数据，以__TMP开头的Key，&lt;/p&gt;

&lt;p&gt;这些Key在运算执行结束后，需要清理掉，我们将这些Key都记录在recycle数据里。&lt;/p&gt;

&lt;p&gt;下面看一个表达式解析的过程：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(C1 = 3 &amp;amp; C2 = 1) &amp;amp; (C5 = 1 | C8 = 6)

--------------------
C1	=	3
C2	=	1
table: 0x41760200	&amp;amp;	table: 0x417553e0
C5	=	1
C8	=	6
table: 0x417600f0	|	table: 0x41760178
table: 0x4175db68	&amp;amp;	table: 0x4175da68

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;再看看解析的结果：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lastKey : {&quot;t&quot; , &quot;__TMP-3&quot;}
process : [
           [&quot;BITOP&quot;,&quot;AND&quot;,&quot;__TMP-1&quot;,&quot;b&quot;,&quot;1-3&quot;,&quot;b&quot;,&quot;2-1&quot;],
           [&quot;BITOP&quot;,&quot;OR&quot;,&quot;__TMP-2&quot;,&quot;b&quot;,&quot;5-1&quot;,&quot;b&quot;,&quot;8-6&quot;],
           [&quot;BITOP&quot;,&quot;AND&quot;,&quot;__TMP-3&quot;,&quot;t&quot;,&quot;__TMP-1&quot;,&quot;t&quot;,&quot;__TMP-2&quot;],
           [&quot;BITCOUNT&quot;,[&quot;t&quot;,&quot;__TMP-3&quot;]]
          ]
recycle : [&quot;__TMP-1&quot;,&quot;__TMP-2&quot;,&quot;__TMP-3&quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;说明：其中的”t”代表是临时变量的key ， “b”代表的是需要结合Block块的序号的Key。&lt;/p&gt;

&lt;p&gt;如果完善一些的话，我们应该对逻辑表达式进行一下处理，也就是执行计划的优化。&lt;/p&gt;

&lt;p&gt;这里只是一个Demo，所以我们先跳过这一块，先把流程跑通。&lt;/p&gt;

&lt;h2 id=&quot;redis&quot;&gt;将执行计划推送给Redis&lt;/h2&gt;

&lt;p&gt;前一篇Blog，我们已经写了一个封装查询的LuaScript了，LuaScript会根据&lt;/p&gt;

&lt;p&gt;Nest结构中的数据，遍历所有的Block块，在每个Block块中进行，逻辑表达式的运算，&lt;/p&gt;

&lt;p&gt;最后把UV都SUM起来，就是返回的日活结果。&lt;/p&gt;

&lt;p&gt;这里我将process和recycle的数据都打包成了json(redis的lua可以使用cjson)。&lt;/p&gt;

&lt;p&gt;下面是重写的ReadScript&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;local scriptRead = redis:script(&quot;LOAD&quot; , 
[[
  local result = 0
  local tableName , dateTime = KEYS[1] , KEYS[2]
  local process , recycle    = cjson.decode(ARGV[1]) , cjson.decode(ARGV[2])
  local key4N = table.concat({tableName , &quot;-&quot; , dateTime , &quot;-Nest&quot;})

  for index , value in ipairs(process) do
    local params
    if value[1] == &quot;BITOP&quot; then
      params = {value[1] , value[2] , value[3]}
      for seq = 4 , #value , 2 do
        if value[seq] == &quot;b&quot; then
          table.insert(params , {tableName , &quot;-&quot; , dateTime , &quot;-&quot; , value[seq + 1] , &quot;-BMP-&quot; , 7})
        elseif value[seq] == &quot;t&quot; then
          table.insert(params , value[seq + 1])
        end
      end
    elseif value[1] == &quot;BITCOUNT&quot; then
      params = {value[1]}
      if value[2] == &quot;b&quot; then
          table.insert(params , {tableName , &quot;-&quot; , dateTime , &quot;-&quot; , value[3] , &quot;-BMP-&quot; , 7})
        elseif value[2] == &quot;t&quot; then
          table.insert(params , value[3])
        end
    end
    process[index] = params
  end
  
  local bytepos = 0
  local pos = redis.call(&quot;bitpos&quot; , key4N , 1 , bytepos)
  while (pos &amp;gt;= 0)
  do
    for g = pos , pos + 7 - bit.band(pos , 7) do
      local fill = redis.call(&quot;getbit&quot; , key4N , g)
      if fill == 1 then
        for index , value in ipairs(process) do
          local ps = {}
          for id , item in ipairs(value) do
            if type(item) == &quot;table&quot; then
              item[7] = g
              ps[id] = table.concat(item)
            else
              ps[id] = item
            end
          end
          if ps[1] == &quot;BITCOUNT&quot; then
            result = result + redis.call(unpack(ps))
          else
            redis.call(unpack(ps))
          end
        end
      end
    end
    bytepos = bit.rshift(pos , 3) + 1 
    pos = redis.call(&quot;bitpos&quot; , key4N , 1 , bytepos)
  end
  if #recycle &amp;gt; 0 then
    redis.call(&quot;del&quot; , unpack(recycle))
  end
  return result
]]
)

scripts:set(&quot;ReadScript&quot; , scriptRead)
ngx.say(&quot;ReadScript : &quot; .. scriptRead)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;最后我们再改写一些Openresty对查询接口的封装&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;G:match(expression)
table.insert(process , {&quot;BITCOUNT&quot; , unpack(lastKey)}) 

local r = redis:evalsha(scripts:get(&quot;ReadScript&quot;) , 2 , tableName , dateTime , json.encode(process) , json.encode(recycle))
ngx.say(r)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 20 Sep 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-09-20-redis-bit2</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-09-20-redis-bit2</guid>
        
        
        <category>redis</category>
        
        <category>lua</category>
        
        <category>bitmap</category>
        
        <category>lpeg</category>
        
      </item>
    
      <item>
        <title>Redis中的Bitmap数据结构</title>
        <description>&lt;p&gt;Redis最早被大家熟知，是因为其高效的读写速率和丰富的数据结构。&lt;/p&gt;

&lt;p&gt;无论是Web业务中做缓存和计数器，还是流计算做数据暂存容器，&lt;/p&gt;

&lt;p&gt;应用都十分广泛。尤其是Bit有关的操作，在统计类业务中颇受欢迎。&lt;/p&gt;

&lt;h2 id=&quot;redisbit&quot;&gt;Redis中常见的Bit操作&lt;/h2&gt;

&lt;p&gt;1、SetBit （写入）&lt;/p&gt;

&lt;p&gt;2、GetBit （读取）&lt;/p&gt;

&lt;p&gt;3、BitOp  （逻辑运算）&lt;/p&gt;

&lt;p&gt;4、BitPos （迭代）&lt;/p&gt;

&lt;p&gt;5、BitField （特殊需求）&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;计算日活跃用户&lt;/h2&gt;

&lt;p&gt;计算日活是一个非常古老的话题，也是面试题目中很常见的一个问题。&lt;/p&gt;

&lt;p&gt;其变种问题有：有限内存的数据排序，在线用户统计等等。&lt;/p&gt;

&lt;p&gt;在网上有很多文章介绍如何利用Redis做高效的日活统计，请自行阅读。&lt;/p&gt;

&lt;p&gt;使用Bit结构作为数据的存储，好处是高效、节省内存。&lt;/p&gt;

&lt;p&gt;之前我写过一篇有关Java的Roaringbitmap的Blog，可以读一下。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;维度分析的需求&lt;/h2&gt;

&lt;p&gt;单一的日活跃统计已经不是什么技术难题了，几乎每个网站或者App都有。&lt;/p&gt;

&lt;p&gt;在实际工作中，运营人员希望看到的数据不是一个简单的统计值，而是能做&lt;/p&gt;

&lt;p&gt;一些更为复杂的统计分析。比如，漏斗模型、多维度组合分析等等。&lt;/p&gt;

&lt;p&gt;通过分析来指导业务的优化，策略的修正。&lt;/p&gt;

&lt;p&gt;现在也有很多公司已经积累了大量的用户特征数据，开始完善用户的画像，&lt;/p&gt;

&lt;p&gt;通过用户的特征来优化业务逻辑，提升用户体验，提高商业价值。&lt;/p&gt;

&lt;p&gt;这类需求简单概括一下就是，我们需要一个列式的存储，方便增加新列，&lt;/p&gt;

&lt;p&gt;需要类似Bitmap的索引，能够高效完成多个维度的快速筛选。&lt;/p&gt;

&lt;p&gt;以Bitmap索引为核心技术的Druid，是一个非常好的选择，被众多互联网&lt;/p&gt;

&lt;p&gt;公司部署，用来做运营分析、广告分析、实时监控。Druid有良好的扩展性，&lt;/p&gt;

&lt;p&gt;延迟比较低，实时强，查询速度快，在多维度分析里表现尤其优异。&lt;/p&gt;

&lt;p&gt;缺点就是架构比较复杂，硬件成本比较高。也有公司选择ES和Kylin等。&lt;/p&gt;

&lt;h2 id=&quot;redis-and-lua&quot;&gt;Redis AND Lua&lt;/h2&gt;

&lt;p&gt;Redis支持加载Lua脚本，让一些原生难以满足的需求可以更优雅的实现。&lt;/p&gt;

&lt;p&gt;使用过Redis的人都知道，通过批量提交可以提升写入效率，假如我们的需求&lt;/p&gt;

&lt;p&gt;是先Get，根据业务逻辑再Put，就很难批量执行，很难保证原子性。&lt;/p&gt;

&lt;p&gt;如果你碰到了迭代的需求，需要反复查询Redis，那么效率也会大打折扣。&lt;/p&gt;

&lt;p&gt;Redis提供的LuaScript扩展是一个非常不错的选择，我们可以把业务逻辑&lt;/p&gt;

&lt;p&gt;封装在Lua的函数中，把业务逻辑放在离数据更近的地方，类似Hbase的&lt;/p&gt;

&lt;p&gt;协处理器的思路，而且Redis本身是单线程的，就不用再处理原子性问题了。&lt;/p&gt;

&lt;p&gt;网上有很多Redis调Lua的例子，这里我就不详细介绍了。&lt;/p&gt;

&lt;p&gt;Redis提供了Bit操作，又是典型的内存数据存储，如果数据量不是特别大，&lt;/p&gt;

&lt;p&gt;能不能用Redis+Lua实现多维度分析日活的功能呢？下面我进行一些尝试。&lt;/p&gt;

&lt;h2 id=&quot;bitmap&quot;&gt;如何构建Bitmap索引&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;用户标识&lt;/h3&gt;

&lt;p&gt;因为是分析日活跃用户，所以采用UserId作为唯一标识，有一些公司的UserId&lt;/p&gt;

&lt;p&gt;是UUID，那就需要进行一次转化，考虑到节省Bitmap的问题，ID尽量连续。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;维度值&lt;/h3&gt;

&lt;p&gt;一个用户身上可能有十几个甚至几十个维度属性值，原则上每个值都要单独构建&lt;/p&gt;

&lt;p&gt;一个Bitmap，所以还需要维护维度的Value列表。如果维度值过于丰富就会导致&lt;/p&gt;

&lt;p&gt;巨大的存储开销。之前在看Kylin的Blog时，注意到有用一个TrieTree的结构&lt;/p&gt;

&lt;p&gt;维度Value字典信息，好处是编码ID的信息具备ASCII的连续性，对Between逻辑&lt;/p&gt;

&lt;p&gt;的执行优化有帮助。在Redis里维护TrieTree有点麻烦，我们暂时用Set替代。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Key的结构是：T-(命名空间)-(日期)-(维度序号)-VS
Value是：Redis的Sets
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;维度值维护在Set里，通过控制Set的大小，来限制膨胀的发生，如果这种逻辑&lt;/p&gt;

&lt;p&gt;在外面已经处理过了，这里也可以简化掉，直接进行下一步操作。&lt;/p&gt;

&lt;h3 id=&quot;bitmap-1&quot;&gt;分块的Bitmap&lt;/h3&gt;

&lt;p&gt;假如我们有一亿个用户ID，每个维度值都维护容量一亿的Bitmap就非常大了。&lt;/p&gt;

&lt;p&gt;为了减小体积，我们将整体用户进行切分。切分的粒度小有助于节省内存，&lt;/p&gt;

&lt;p&gt;但是会导致查询性能变差，粒度大则会浪费更多的内存。如果Redis的Bitmap&lt;/p&gt;

&lt;p&gt;能够有Roaringbitmap的优化效果就完美了，对Bitmap的压缩可以参考一下&lt;/p&gt;

&lt;p&gt;Roaringbitmap的源码，这里就不细说了。&lt;/p&gt;

&lt;p&gt;块的大小可以根据自己的数据规模来确定，一般选择16384或者65536等。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;blockSeq , lineSeq = bit.rshift(uidNumber , 14) , bit.band(uidNumber , 16383)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;之所以取2的N次方，是为了更快的处理分块的序号和偏移，利用位移和与运算。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Key的结构是：T-(命名空间)-(日期)-(维度序号)-(维度值)-BMP-(块序号)
Value是：Redis的Bit
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;因为做了分块的操作，所以我们还需要维护一个块信息的列表，方便我们遍历计算。&lt;/p&gt;

&lt;p&gt;一般来说会在这个块信息列表上存一些统计信息，比如总数。如果没有什么附加信息的话，&lt;/p&gt;

&lt;p&gt;我们仍然可以用Bitmap来维护这个列表，节省一点内存开销。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Key的结构是：T-(命名空间)-(日期)-Nest
Value是：Redis的Bit
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;题外话：Sybase之前有一个专利叫Bit-Wise，可以解决高基维和Between问题，&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.infocenter.dc38159.1600/doc/html/rhi1334843412230.html&quot;&gt;参考1&lt;/a&gt;
&lt;a href=&quot;http://blog.163.com/liaoxiangui@126/blog/static/795696402012101510276877/&quot;&gt;参考2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;今后我们再讨论这个话题。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;如何查询&lt;/h2&gt;

&lt;p&gt;一般查询的条件就是 col1 = 3 and col4 = true and col10 = abc 这个样子。&lt;/p&gt;

&lt;p&gt;所以我们要根据维度的序号和维度的值，找到对应的BitmapKey,参见之前的key结构。&lt;/p&gt;

&lt;p&gt;最后我们要根据分块的列表进行遍历，汇总遍历的结果返回。&lt;/p&gt;

&lt;p&gt;伪代码如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for index in blocksArray
  AND result col1-3-index col4-true-index col10-abc-index
end
bitcount result 
del result
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;遍历Block列表的时候，我用到了BitPos，这个操作的参数非常麻烦，&lt;/p&gt;

&lt;p&gt;参数是byte级别的，返回值是bit，需要自己加逻辑去控制一下。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;local scripts = ngx.shared.scripts
local shar = redis:script(&quot;LOAD&quot; , 
[[
  local result = 0
  local tableName , dateTime = KEYS[1] , KEYS[2]
  
  local keyT4N = {&quot;T-&quot; , tableName , &quot;-&quot; , dateTime , &quot;-Nest&quot;}
  local key4N = table.concat(keyT4N)

  local subKeys , subKeysT = {} , {}
  for index = 1 , #ARGV , 2 do
    table.insert(subKeysT , {&quot;T-&quot; , tableName , &quot;-&quot; , dateTime , &quot;-&quot; , ARGV[index] , &quot;-&quot; , ARGV[index+1] , &quot;-BMP-&quot; , 10})
  end

  local bytepos = 0
  local pos = redis.call(&quot;bitpos&quot; , key4N , 1 , bytepos)
  while (pos &amp;gt;= 0)
  do
    for g = pos , pos + 7 - bit.band(pos , 7) do
      local fill = redis.call(&quot;getbit&quot; , key4N , g)
      if fill == 1 then
        for index , val in ipairs(subKeysT) do
          val[10] = g
          subKeys[index] = table.concat(val)
        end
        redis.call(&quot;bitop&quot; , &quot;AND&quot; , &quot;_TMP_&quot; , unpack(subKeys))
        local uv = redis.call(&quot;bitcount&quot; , &quot;_TMP_&quot;)
        result = result + uv
      end
    end

    bytepos = bit.rshift(pos , 3) + 1 
    pos = redis.call(&quot;bitpos&quot; , key4N , 1 , bytepos)
  end
  redis.call(&quot;del&quot; , &quot;_TMP_&quot;)
  return result
]]
)
scripts:set(&quot;ReadScript&quot;, shar)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;section-5&quot;&gt;数据写入&lt;/h2&gt;

&lt;p&gt;其实就是将行转列的过程&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;local scriptWrite = redis:script(&quot;LOAD&quot; , 
[[
  local blockSize , blockPower , intSize = 2^15 , 15 , 2^31
  local expireTime = 7 * 86400

  local tableName , dateTime , uidNumber , append = KEYS[1] , tonumber(KEYS[2]) , tonumber(KEYS[3]) , KEYS[4] == &quot;1&quot;
  local expire = (dateTime &amp;gt; 0)
  local blockSeq , lineSeq 

  if uidNumber &amp;gt;= intSize then
    blockSeq , lineSeq = math.modf(uidNumber / blockSize) , uidNumber % blockSize
  else
    blockSeq , lineSeq = bit.rshift(uidNumber , blockPower) , bit.band(uidNumber , blockSize - 1)
  end
  
  local key4N  = table.concat({tableName , &quot;-&quot; , dateTime , &quot;-Nest&quot;})
  local keyT4V = {tableName , &quot;-&quot; , dateTime , &quot;-&quot; , 5 , &quot;-&quot; , &quot;VS&quot;}
  local keyT4M = {tableName , &quot;-&quot; , dateTime , &quot;-&quot; , 5 , &quot;-&quot; , 7 , &quot;-BMP-&quot; , blockSeq}
  
  local write = function(keyT4V , keyT4M , index , val , lineSeq , expireTime)
    local oldBit
    keyT4V[5] , keyT4M[5] = index , index
    local key4V = table.concat(keyT4V)
    local isExist = redis.call(&quot;SISMEMBER&quot; , key4V , val)
    if isExist == 1 then
      keyT4M[7] = val
      local key4M = table.concat(keyT4M)
      oldBit = redis.call(&quot;setbit&quot; , key4M , lineSeq , 1)  
    else
      local count = redis.call(&quot;SCARD&quot; , key4V)
      if count &amp;gt;= 1024 then
        val = &quot;IGNORE&quot;
      end
      if count &amp;lt;= 1024 then 
        redis.call(&quot;SADD&quot; , key4V , val)
      end      
      if count == 0 and expire then
        redis.call(&quot;expire&quot; , key4V , expireTime)
      end
      keyT4M[7] = val
      local key4M = table.concat(keyT4M)
      oldBit = redis.call(&quot;setbit&quot; , key4M , lineSeq , 1)
      if expire then
        redis.call(&quot;expire&quot; , key4M , expireTime)
      end
    end
    return oldBit
  end

  local oldBitNum = 0
  if append then  
    for seq = 1 , #ARGV , 2 do
      oldBitNum = oldBitNum + write(keyT4V , keyT4M , ARGV[seq] , ARGV[seq + 1] , lineSeq , expireTime)
    end
  else
    for index , val in ipairs(ARGV) do
      oldBitNum = oldBitNum + write(keyT4V , keyT4M , index , val , lineSeq , expireTime) 
    end
  end
  if oldBitNum == 0 then
    redis.call(&quot;setbit&quot; , key4N , blockSeq , 1)
  end
  return true
]]
)
scripts:set(&quot;WriteScript&quot;, scriptWrite)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;OpenrestyLua包装的查询接口&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;local ngx_re      = require &quot;ngx.re&quot;
local resty_redis = require &quot;resty.redis&quot;
local redis       = resty_redis:new()
redis:set_timeout(1000)
local ok , err    = redis:connect(&quot;127.0.0.1&quot;, 6379)

if err then
  ngx.say(err)
  return
end

local scripts = ngx.shared.scripts

local tableName = ngx.var.arg_table
local dateTime  = ngx.var.arg_datetime

local dimsStr   = ngx.var.arg_dims
local dims      = ngx_re.split(dimsStr , &quot;,&quot;)

local r = redis:evalsha(scripts:get(&quot;ReadScript&quot;) , 2 , tableName , dateTime , unpack(dims))
ngx.say(r)
redis:close()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 12 Sep 2017 09:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-09-12-redis-bit</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-09-12-redis-bit</guid>
        
        
        <category>redis</category>
        
        <category>lua</category>
        
        <category>bitmap</category>
        
      </item>
    
      <item>
        <title>Lpeg与模式匹配</title>
        <description>&lt;p&gt;去年年底，开发第一版logwatch的时候，就有关注到Lpeg这个项目。&lt;/p&gt;

&lt;p&gt;调研期间看了mozilla的heka项目，在解析日志时引入了lua的sandbox，&lt;/p&gt;

&lt;p&gt;其定义的解析函数主要依赖Lpeg来实现，（LPEG可以称为PEG的Lua实现）。&lt;/p&gt;

&lt;p&gt;PEG相关知识自行google–&amp;gt;解析表达文法。&lt;/p&gt;

&lt;h2 id=&quot;lpeg&quot;&gt;日志领域的Lpeg&lt;/h2&gt;

&lt;p&gt;为什么mozilla的heka要引入lua和lpeg呢？&lt;/p&gt;

&lt;p&gt;heka是基于go开发的高性能日志处理工具，lua也是以性能著称的，&lt;/p&gt;

&lt;p&gt;而且增加或者修改lua文件并不需要重新编译，非常理想的插件模式。&lt;/p&gt;

&lt;p&gt;对于一般的日志文件解析来说正则就足够强大了，但这个世界并不完美，&lt;/p&gt;

&lt;p&gt;总有一些特殊情况，导致了复杂度。假如日志文件里存在一种以上的格式，&lt;/p&gt;

&lt;p&gt;如何高效的进行解析？如何解析抽取UserAgent这样的字段？&lt;/p&gt;

&lt;p&gt;模式匹配是一个很不错的选择，lua领域的lpeg就可以完成这个工作。&lt;/p&gt;

&lt;p&gt;几年前我在看一个解析UserAgent的java库，大概实现思路是要跑N条正则，&lt;/p&gt;

&lt;p&gt;看看哪个可以完成解析，这样的效率显然是很低。&lt;/p&gt;

&lt;h2 id=&quot;lpeg-1&quot;&gt;初识Lpeg&lt;/h2&gt;

&lt;p&gt;Lpeg的入门还是有一些难度的，语法比较怪异，相关的例子也不是很多。&lt;/p&gt;

&lt;p&gt;我在git上建了一个项目，记录了我练习的一些例子，从最简单的计算器、&lt;/p&gt;

&lt;p&gt;到一个日期格式自动识别转化的工具、再到一个简易的模板(支持变量替&lt;/p&gt;

&lt;p&gt;换和for循环语句)。&lt;a href=&quot;https://github.com/peiliping/lpeg-test/tree/master/basic&quot;&gt;【lpeg-test】&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;如果你想用lpeg来自制一门脚本语言parser的话，还是要借助一些高级库，&lt;/p&gt;

&lt;p&gt;比如&lt;a href=&quot;https://github.com/sqmedeiros/lpeglabel&quot;&gt;【label】&lt;/a&gt;、&lt;a href=&quot;https://github.com/andremm/lua-parser&quot;&gt;【parser】&lt;/a&gt;、&lt;a href=&quot;https://github.com/luapower/lexer&quot;&gt;【lexer】&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Sep 2017 09:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-09-01-lpeg</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-09-01-lpeg</guid>
        
        
        <category>lpeg</category>
        
        <category>lua</category>
        
        <category>模式匹配</category>
        
      </item>
    
      <item>
        <title>关于性能优化</title>
        <description>&lt;p&gt;最近两周一直在对去年开发的logwatch进行重构和升级，修复一些小bug、&lt;/p&gt;

&lt;p&gt;扩展了对文件名的正则支持、升级了相关的三方依赖库，重新规划了目录结构，&lt;/p&gt;

&lt;p&gt;当然，优化性能也是很重要的一个目标。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;越来越难的性能优化&lt;/h2&gt;

&lt;p&gt;去年开发logwatch替换logstash的时候，就是因为logstash的性能差。&lt;/p&gt;

&lt;p&gt;所以着重在优化logwatch的性能上，上线的版本可以达到每秒5万行NginxLog。&lt;/p&gt;

&lt;p&gt;当时主要的优化手段是提高正则解析的效率，以最低的回溯代价完成解析。&lt;/p&gt;

&lt;p&gt;而后在一些关键数据结构上做到重用，大大降低了table的性能开销。&lt;/p&gt;

&lt;p&gt;最后有针对性的优化了KafkaClient的参数，保证高效率的消息传递。&lt;/p&gt;

&lt;p&gt;这次再要优化性能已经非常困难了，从profiler的统计看，热点已经不在Lua了。&lt;/p&gt;

&lt;p&gt;所以优化的方向主要是提升依赖的三方库的性能，优化的目标是7万以上。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;三方依赖&lt;/h2&gt;

&lt;p&gt;最吃cpu的三方库就是cjson，花了点时间寻找性能更好的json库。&lt;/p&gt;

&lt;p&gt;参考了网上的一些测试结果，和自己的简单测试，发现rapidjson是最快的。&lt;/p&gt;

&lt;p&gt;rapidjson相关的资料在网上可以搜到，大概比cjson快了一倍多。&lt;/p&gt;

&lt;p&gt;因为lua本身提供的基础库非常有限，甚至连Sleep这样的功能都没有原生提供。&lt;/p&gt;

&lt;p&gt;这次升级和优化时，也将一些通过os.exec来执行的命令替换成相关的三方库。&lt;/p&gt;

&lt;p&gt;比如，引入了LuaFileSystem来丰富对目录和文件的操作，通过ffi来实现sleep。&lt;/p&gt;

&lt;p&gt;虽然这些改进不能明显的提高性能，但是可以降低吞吐量的波动性。&lt;/p&gt;

&lt;p&gt;在数据压缩方面由snappy换成了lz4，测试结果表明lz4还是会更快一些。&lt;/p&gt;

&lt;p&gt;修改kafka的参数socket.blocking.max.ms也可以提高一些性能，测试结果显示&lt;/p&gt;

&lt;p&gt;80-100之间是最佳状态。&lt;/p&gt;

&lt;p&gt;关于Lua的Ipairs和Pairs网上有很多介绍的资料了，性能上ipairs肯定是更好的，&lt;/p&gt;

&lt;p&gt;但是pairs的操作更方便，代码更整洁。所以要根据情况来取舍。&lt;/p&gt;

&lt;p&gt;高密度执行的语句中，尽量使用ipairs，反之可以用pairs。&lt;/p&gt;

&lt;p&gt;经过一番折腾，可以在理想环境(haswell或者broadwell的cpu、网络延迟低)下，&lt;/p&gt;

&lt;p&gt;达到每秒8万以上的吞吐量，一般情况也可以超过7万。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;是否过度优化&lt;/h2&gt;

&lt;p&gt;跟朋友讨论时也谈到logwatch是不是过度优化了性能，应该去实现更多的功能，&lt;/p&gt;

&lt;p&gt;很少有业务能把日志打印量做到每秒几万行，这样的性能几乎无用武之地。&lt;/p&gt;

&lt;p&gt;这里谈一下我的看法：&lt;/p&gt;

&lt;p&gt;性能测试都会基于一定的前提或者一定的测试数据，多少会有一些局限性。&lt;/p&gt;

&lt;p&gt;我的测试数据是来自生产环境上的nginxlog，每行均长是200字节。&lt;/p&gt;

&lt;p&gt;这个测试数据的标准不算很高，我问过几家公司的朋友他们的均长在300左右。&lt;/p&gt;

&lt;p&gt;因为业务的差别和logformat的差异，每家都会不太一样。&lt;/p&gt;

&lt;p&gt;当然即使是300的均长，logwatch的吞吐能力也在6W以上，足够满足需求。&lt;/p&gt;

&lt;p&gt;日志采集的探针是一个小组件，每个人都希望他是低负担的，低成本的。&lt;/p&gt;

&lt;p&gt;如果性能做的好，在部署上就不会带来任何负担。比如，不用单独为它准备&lt;/p&gt;

&lt;p&gt;一个cpucore，或者扩充系统内存之类的。logwatch在生产环境下，&lt;/p&gt;

&lt;p&gt;每秒处理500行log，占用24m内存，单核的百分之1.5的cpu。&lt;/p&gt;

&lt;p&gt;不一定每个使用它的场景都会有那么好的cpu和网络，这些不确定性也会导致&lt;/p&gt;

&lt;p&gt;性能有所下降，提升性能会让它适应能力更强。&lt;/p&gt;

&lt;p&gt;网上看过很多日志采集的方案，最近一年多比较流行的方案是在本地只做基本的&lt;/p&gt;

&lt;p&gt;行和多行切分，其他的解析操作全部扔到后面去做，比如Kafka的Consumer。&lt;/p&gt;

&lt;p&gt;这当然是一个不错的方案，但是将如此重Cpu的工作全部放在后端处理，意味着&lt;/p&gt;

&lt;p&gt;你需要一个比较大的资源池，也会有一些相应的管理成本。&lt;/p&gt;

&lt;p&gt;在我看来适当的将日志解析的工作放在前端agent执行，是更经济的做法。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;最后&lt;/h2&gt;

&lt;p&gt;任何一种选型或者方案都不会是完美的，要看你对哪一方面更敏感，&lt;/p&gt;

&lt;p&gt;明确知道这个方案的边界，尽量避免触礁，被滥用是很多项目失败的根本原因。&lt;/p&gt;

</description>
        <pubDate>Wed, 09 Aug 2017 09:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-08-09-performance</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-08-09-performance</guid>
        
        
        <category>性能</category>
        
        <category>优化</category>
        
        <category>logwatch</category>
        
      </item>
    
      <item>
        <title>支持long型的bitmap</title>
        <description>&lt;p&gt;日常工作中我们使用到bitmap的场景并不是很多，前几年在面试中倒是经常会被问到，&lt;/p&gt;

&lt;p&gt;比如，一亿用户id的去重、排序、布隆过滤器等等，分析型数据库中bitmap也是利器。&lt;/p&gt;

&lt;p&gt;Java中常用的bitmap实现有BitSet，还有开源的、功能强大的Roaringbitmap。&lt;/p&gt;

&lt;p&gt;近年比较火的OLAP领域，大量的采用Roaringbitmap作为bitmap的基础实现库。&lt;/p&gt;

&lt;p&gt;在读写速度还有内存控制上Roaringbitmap都非常优秀，但是有一个比较大的限制，&lt;/p&gt;

&lt;p&gt;就是只支持int类型的数据写入，而实际工作中，我们想要存入Bitmap的是long型。&lt;/p&gt;

&lt;p&gt;如果你去搜索bitmap三方库的issue就会发现，有此类需求的人非常多。&lt;/p&gt;

&lt;p&gt;众多开源项目在使用Roaringbitmap的时候，也碰到了这问题，有的是扩展实现了对&lt;/p&gt;

&lt;p&gt;long型的支持，有的是做了预处理，将数据控制在int类型范围内。&lt;/p&gt;

&lt;p&gt;之前在meepo项目中引入bitmap，为了解决在两张表中，查找差异的主键ID，&lt;/p&gt;

&lt;p&gt;一般来说一个表的主键ID都是自增的long型(bigint)，所以要扩展long型的支持。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;思考&lt;/h2&gt;

&lt;p&gt;刚才提到bitmap经常被用来存储ID，那么在业务中ID为什么经常被定义为long型呢？&lt;/p&gt;

&lt;p&gt;我们平时使用的大多数表数据量可能只有几百或者几千，这些数据也很少更新和增加。&lt;/p&gt;

&lt;p&gt;这种数据可以称为常量数据，比如定义浏览器、操作系统、运营商等等，这种场景完全&lt;/p&gt;

&lt;p&gt;可以使用int、甚至更短的整数形式，有利于减低索引的开销，还有运行时的内存占用。&lt;/p&gt;

&lt;p&gt;还有一种ID是流水号，比如订单ID、用户ID、商品ID等等。考虑业务未来的增长，&lt;/p&gt;

&lt;p&gt;有必要把他们定义为long型。当然实际上，绝大多数公司都会在流水号上做一些手脚，&lt;/p&gt;

&lt;p&gt;比如在订单流水号的末尾添加用户ID的后四位，用来作为分库分表的依据。&lt;/p&gt;

&lt;p&gt;为了防止ID被穷举，设置适当的跳跃和偏移也是必要的。总之，ID涉及到拼接，&lt;/p&gt;

&lt;p&gt;就极有可能超过int类型的范围了。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;如何实现&lt;/h2&gt;

&lt;p&gt;如果你真有海量数据需要需要填充到bitmap中，你需要考虑一下内存是不是够用了，&lt;/p&gt;

&lt;p&gt;就算支持到long型，内存不够也处理不了。既然一个bitmap可以支持int，那么&lt;/p&gt;

&lt;p&gt;我们使用多个bitmap，依次表达 N × int。这样可以使用现有的Roaringbitmap，&lt;/p&gt;

&lt;p&gt;作为基础实现。这里有实现代码：&lt;a href=&quot;https://github.com/peiliping/meepo/tree/master/src/main/java/meepo/util/hp&quot;&gt;【BitMap】&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;细节&lt;/h2&gt;

&lt;p&gt;在处理N × int的时候，很多人会使用整除和取模的方式，当然这样可以实现功能，&lt;/p&gt;

&lt;p&gt;但是运行效率一般，参考Ringbuffer在处理Sequence时的技巧，实现如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;long seq = data &amp;gt;&amp;gt; 31;
int val = (int) (data &amp;amp; Integer.MAX_VALUE);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;测试下来，整体运行效率可以提高一倍。&lt;/p&gt;

&lt;p&gt;因为这些Roaringbitmap是有序的，使用Treemap当然是最理想的。&lt;/p&gt;

&lt;p&gt;但是在搜索性能上，Treemap不及Hashmap，所以只在局部使用。&lt;/p&gt;

&lt;p&gt;Roaringbitmap的接口非常多，这里只实现了一部分，后面根据需要再补充。&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Aug 2017 09:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-08-01-64bitmap</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-08-01-64bitmap</guid>
        
        
        <category>java</category>
        
        <category>bitmap</category>
        
      </item>
    
      <item>
        <title>Flume优化</title>
        <description>&lt;p&gt;Flume是Apache大数据套件中比较亲民的，安装、配置简单，功能丰富，而且极易定制化。&lt;/p&gt;

&lt;p&gt;Flume的整体设计也是非常值得学习的。之前美团在官方技术blog中，着重讲述了其在Flume&lt;/p&gt;

&lt;p&gt;上的定制化开发和性能优化，也让Flume普及率大大提高了。&lt;/p&gt;

&lt;p&gt;一般使用Flume主要是作为agent抓取日志或者从Kafka读取消息写入HDFS、Hbase等，&lt;/p&gt;

&lt;p&gt;而作为消息通道的功能，这几年渐渐被性能卓越的Kafka所取代。&lt;/p&gt;

&lt;p&gt;而在日志抓取领域，Flume的功能性和性能一直都不是最佳选择。&lt;/p&gt;

&lt;p&gt;下面介绍一下我在Flume定制开发和性能优化中的一些实践。&lt;/p&gt;

&lt;h2 id=&quot;kafkasourcesink&quot;&gt;KafkaSourceSink&lt;/h2&gt;

&lt;p&gt;在我的应用场景中，Flume主要是从Kafka拉取数据写入到HDFS上，还有就是从Kafka到Kafka。&lt;/p&gt;

&lt;p&gt;Kafka这几年的发展很快，从比较流行的0.8.2.2到现在的0.10.X版本，API的兼容不是很好。&lt;/p&gt;

&lt;p&gt;Flume的官方版本支持的是0.9.X，需要定制一下Kafka的版本支持。把KafkaSource和Sink&lt;/p&gt;

&lt;p&gt;中,跟KafkaAPI打交道的方法都封装在抽象类中，提供多种版本的实现，这样就具备了不同Kafka&lt;/p&gt;

&lt;p&gt;版本之间的消息迁移，主要用途就是在业务升级Kafka版本时，作为辅助的数据迁移工具，&lt;/p&gt;

&lt;p&gt;业务方可以分别升级Producer端和Consumer端，减少升级的难度，控制升级的风险。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;优化性能&lt;/h2&gt;

&lt;p&gt;整体来说Flume的性能并不高，很多组件为了实现通用性，都放弃了性能。&lt;/p&gt;

&lt;p&gt;通过JFR的火焰图，我们找到了一些可以优化的点，下面列举一些。&lt;/p&gt;

&lt;h3 id=&quot;hdfseventsink&quot;&gt;HDFSEventSink&lt;/h3&gt;

&lt;p&gt;HDFSEventSink最大的问题是HDFSPath路径的渲染，将EventHeader里的Tag写入Path路径中，&lt;/p&gt;

&lt;p&gt;还有一些预设的变量，比如年月日时分秒。这种渲染逻辑是每条数据都要执行的，所以开销非常大。&lt;/p&gt;

&lt;p&gt;解决办法：&lt;/p&gt;

&lt;p&gt;尽量少的使用变量，可以在Source端将路径的相关信息都拼凑好，写成一两个tag，&lt;/p&gt;

&lt;p&gt;最后在sink端做简单的字符串拼接，性能至少可以提升两三倍。&lt;/p&gt;

&lt;p&gt;注意：日期格式的渲染也要考虑，如果每条数据都将timestamp格式化成YMD也是非常耗时的，&lt;/p&gt;

&lt;p&gt;可以适当引入cache来解决这个问题，比如将timestamp整除60000，缓存一分钟的格式化结果。&lt;/p&gt;

&lt;p&gt;网上见到很多人的flume例子中，都是写TextFile到HDFS的，我是选择写的SequenceFile，&lt;/p&gt;

&lt;p&gt;并且加了Snappy的压缩。SequenceFile比TextFile的好处就不讲了，网上有很多介绍。&lt;/p&gt;

&lt;p&gt;另外sequenceFile也有一点点可以改的地方，将HDFSWritableSerializer的KeyClass修改掉，&lt;/p&gt;

&lt;p&gt;Flume源码里是LongWritable，可以改成NullWritable。修改过的SequenceFile并不影响使用，&lt;/p&gt;

&lt;p&gt;HDFS的文件大小会有百分之X的下降，相应的网络传输也会有一些收益。&lt;/p&gt;

&lt;h3 id=&quot;eventsimpleevent&quot;&gt;Event、SimpleEvent&lt;/h3&gt;

&lt;p&gt;在Flume体系中，Event是数据的载体，一共有两部分组成，一个Header，一个Body。&lt;/p&gt;

&lt;p&gt;Header是一个HashMap，这里的HashMap是可以优化的，参见我上一篇文章吧。&lt;/p&gt;

&lt;p&gt;这里有一个陷阱需要注意，SimpleEvent的实现中，Header是每次创建时就new了一个HashMap。&lt;/p&gt;

&lt;p&gt;按照Flume源码中Kafkasource的逻辑EventBuilder.withBody(eventBody, headers)，&lt;/p&gt;

&lt;p&gt;你在外面构建的headers(map),会在event初始化时复制一遍，这里可以优化一下。&lt;/p&gt;

&lt;h3 id=&quot;channelprocessor&quot;&gt;ChannelProcessor&lt;/h3&gt;

&lt;p&gt;ChannelProcessor的实现比较丰富，但不一定你都需要用到，这里首先可以做了一下裁剪。&lt;/p&gt;

&lt;p&gt;在我的Flume中不会用到optional和InterceptorChain，所以先将相关逻辑删掉了。&lt;/p&gt;

&lt;p&gt;processEventBatch方法的参数events，在下面的实现逻辑中被复制到了reqChannelQueue里，&lt;/p&gt;

&lt;p&gt;在我的Source中，一个batch操作的数据都是指向同一个channel的，所以这个复制也没有意义。&lt;/p&gt;

&lt;p&gt;只需要判断第一个元素的header，选择req的Channel，直接执行Put即可。&lt;/p&gt;

&lt;p&gt;如果一个batch的数据可能是流向不同channel的，那么尽量在source端拆分好。&lt;/p&gt;

&lt;p&gt;source端承载数据的list是可以反复重用的，而且capacity也可以预设大小，避免resize。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;最后&lt;/h3&gt;

&lt;p&gt;写好的代码如果你是以plugin的形式来开发flume的话，那上面的部分代码是没法生效的。&lt;/p&gt;

&lt;p&gt;比如ChannelProcessor，有一个简单的办法，在不用修改Flume源码的情况下，就可以生效。&lt;/p&gt;

&lt;p&gt;找到Flume/bin目录下的flume-ng启动脚本，在这里修改一下java -cp 后面的参数顺序，&lt;/p&gt;

&lt;p&gt;保证你的plugin目录，在flume/lib目录之前，就可以优先加载所覆写的类了。&lt;/p&gt;
</description>
        <pubDate>Wed, 05 Jul 2017 10:00:00 +0800</pubDate>
        <link>http://peiliping.github.io/blog/archivers/2017-07-05-flume</link>
        <guid isPermaLink="true">http://peiliping.github.io/blog/archivers/2017-07-05-flume</guid>
        
        
        <category>flume</category>
        
        <category>java</category>
        
      </item>
    
  </channel>
</rss>
